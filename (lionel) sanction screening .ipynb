{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import dateutil.parser as parser\n",
    "from geopy.geocoders import Nominatim\n",
    "import pycountry\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pinyin\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "df = pd.read_excel(\"NUS sample names_V2.xlsx\", engine=\"openpyxl\")\n",
    "df = df.where(pd.notnull(df), None)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/lionel/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Name to be screened must be in English\n",
    "# Alias names can only handle Chinese characters , else return None\n",
    "def preprocess_df_to_dict(df):\n",
    "    def get_year(date):\n",
    "        try:\n",
    "            parser_obj = parser.parse(str(date))\n",
    "            return parser_obj.year\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_month(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).month\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def get_day(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).day\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "    \n",
    "    df_dict_list = df.to_dict('records')\n",
    "    cleaned_dict_list = []\n",
    "    for record in df_dict_list:\n",
    "        \n",
    "        alias = record['Alias name']\n",
    "        if alias is not None:\n",
    "            alias_is_english = isEnglish(alias)\n",
    "            if alias_is_english is False:\n",
    "                try:\n",
    "                    alias = pinyin.get(alias, format='strip', delimiter=' ')\n",
    "                except:\n",
    "                    alias = None\n",
    "        current_record = {\n",
    "            'name': record['Name to be screened'],\n",
    "            'alias' : alias,\n",
    "            'year_of_birth': get_year(record['Date of birth']),\n",
    "            'month_of_birth': get_month(record['Date of birth']),\n",
    "            'day_of_birth': get_day(record['Date of birth']),\n",
    "            'gender': record['Gender'],\n",
    "            'nationality': record['Nationality'],\n",
    "            ### delete these later on, for testing only###\n",
    "            'type_of_error': record['Type of variation (if any)'],\n",
    "            'actual_name': record['Actual name'],\n",
    "        }\n",
    "        cleaned_dict_list.append(current_record)\n",
    "    return cleaned_dict_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "def ER_name_matching(name1, name2):\n",
    "    def split_name_list(name):\n",
    "        name = name.lower()\n",
    "        output = name.split(\" \")\n",
    "        return output\n",
    "\n",
    "    def preprocess_name(names_dict, word):\n",
    "        for key, value in names_dict.items():\n",
    "            if word in value:\n",
    "                return key\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def stitch_name(list1):\n",
    "        output = ''\n",
    "        for x in range(len(list1)):\n",
    "            if x==0:\n",
    "                output += list1[x]\n",
    "            else:\n",
    "                output += ' ' + list1[x]\n",
    "        return output\n",
    "\n",
    "    def phonetic_comparison(list1, list2):\n",
    "        meta_list1 = []\n",
    "        meta_list2 = []\n",
    "        nysiis_list1 = []\n",
    "        nysiis_list2 = []\n",
    "        for name_1 in list1:\n",
    "            meta_list1.append(jellyfish.metaphone(name_1))\n",
    "            nysiis_list1.append(jellyfish.nysiis(name_1))\n",
    "        for name_2 in list2:\n",
    "            meta_list2.append(jellyfish.metaphone(name_2))\n",
    "            nysiis_list2.append(jellyfish.nysiis(name_2))\n",
    "        if (set(meta_list1) == set(meta_list2)) or (set(nysiis_list1) == set(nysiis_list2)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def excel_to_dict(excel_file):\n",
    "        excel_df = pd.read_excel(excel_file)\n",
    "        excel_df.value.apply(str)\n",
    "        before_transformation = dict(zip(excel_df.key, excel_df.value))\n",
    "        dictionary = {key: [val for val in value.split(',')] for key, value in before_transformation.items()}\n",
    "        return dictionary\n",
    "            \n",
    "    names_dict = excel_to_dict('names_dict.xlsx') \n",
    "    \n",
    "    # START #\n",
    "    ### Change this if needed ###\n",
    "    threshold = 89\n",
    "    #############################\n",
    "    \n",
    "    split_list_1 = split_name_list(name1)\n",
    "    split_list_2 = split_name_list(name2) \n",
    " \n",
    "    # if len(split_list_1) != len(split_list_2):\n",
    "    #     return None\n",
    "    \n",
    "    for i in range(len(split_list_1)):\n",
    "        split_list_1[i] = preprocess_name(names_dict, split_list_1[i])        \n",
    "    for i in range(len(split_list_2)):\n",
    "        split_list_2[i] = preprocess_name(names_dict, split_list_2[i])\n",
    "    \n",
    "    stitched_name1 = stitch_name(split_list_1)\n",
    "    stitched_name2 = stitch_name(split_list_2)\n",
    "    \n",
    "    # 1st layer of testing: Token Sort Ratio with threshold\n",
    "    score1 = fuzz.token_sort_ratio(stitched_name1, stitched_name2)\n",
    "    if score1 >= threshold:\n",
    "        # score_list.append(score1)\n",
    "        return score1\n",
    "        # do something\n",
    "# 4) 2nd layer of testing - Metaphone and NYSIIS phonetic encoding - DONE\n",
    "    else: \n",
    "        try:\n",
    "            matched_phonetic = phonetic_comparison(split_list_1, split_list_2)\n",
    "            if matched_phonetic:\n",
    "                return threshold # assumption that phonetic match will give threshold score\n",
    "            else: \n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    try:\n",
    "        return score1\n",
    "    except:\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df_dict = preprocess_df_to_dict(df)\n",
    "test_record_1 = df_dict[0]\n",
    "test_record_18 = df_dict[15]\n",
    "print(test_record_18)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'name': 'Lange Vivian', 'alias': None, 'year_of_birth': 1997, 'month_of_birth': None, 'day_of_birth': None, 'gender': 'Female', 'nationality': 'Singapore', 'type_of_error': '-', 'actual_name': 'Lange Vivian'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "outputs = []\n",
    "for x in df_dict:\n",
    "    if 'Mun San' in x['name']:\n",
    "        outputs.append(x)\n",
    "        \n",
    "test = outputs[0]\n",
    "test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'Son Mun San',\n",
       " 'alias': None,\n",
       " 'year_of_birth': 1951,\n",
       " 'month_of_birth': None,\n",
       " 'day_of_birth': None,\n",
       " 'gender': '-',\n",
       " 'nationality': None,\n",
       " 'type_of_error': '-',\n",
       " 'actual_name': 'Son Mun San'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ALGO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "import time\n",
    "\n",
    "def sanction_screening(client):\n",
    "    def split_name_list(name):\n",
    "        name = name.lower()\n",
    "        output = name.split(\" \")\n",
    "        return output\n",
    "\n",
    "    def preprocess_name(names_dict, word):\n",
    "        for key, value in names_dict.items():\n",
    "            if word in value:\n",
    "                return key\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def stitch_name(list1):\n",
    "        output = ''\n",
    "        for x in range(len(list1)):\n",
    "            if x==0:\n",
    "                output += list1[x]\n",
    "            else:\n",
    "                output += ' ' + list1[x]\n",
    "        return output\n",
    "    \n",
    "    def excel_to_dict(excel_file):\n",
    "        excel_df = pd.read_excel(excel_file)\n",
    "        excel_df.value.apply(str)\n",
    "        before_transformation = dict(zip(excel_df.key, excel_df.value))\n",
    "        dictionary = {key: [val for val in value.split(',')] for key, value in before_transformation.items()}\n",
    "        return dictionary\n",
    "\n",
    "    start = time.time()\n",
    "    names_dict = excel_to_dict('names_dict.xlsx') \n",
    "    sanction_list_dict = pd.read_csv(\"cleaned_indiv_sanction_list.csv\").to_dict('records')\n",
    "    \n",
    "    client_name = client['name']\n",
    "    split_client_name = split_name_list(client_name)\n",
    "\n",
    "    for i in range(len(split_client_name)):\n",
    "        split_client_name[i] = preprocess_name(names_dict, split_client_name[i])        \n",
    "    stitched_client_name = stitch_name(split_client_name)\n",
    "    \n",
    "    for record in sanction_list_dict:\n",
    "        current_sanc_name = record['name']\n",
    "        split_sanction_name = split_name_list(current_sanc_name)\n",
    "        if len(split_client_name) != len(split_sanction_name):\n",
    "            continue\n",
    "        for i in range(len(split_sanction_name)):\n",
    "            split_sanction_name[i] = preprocess_name(names_dict, split_sanction_name[i])\n",
    "        \n",
    "        stitched_sanc_name = stitch_name(split_sanction_name)\n",
    "        \n",
    "        if abs(len(stitched_client_name) - len(stitched_sanc_name))>3:\n",
    "            # print(stitched_client_name, stitched_sanc_name)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            flag = ER_name_matching(client_name, current_sanc_name)\n",
    "            # print(\"go\")\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if flag is None:\n",
    "                continue\n",
    "            if flag > 0:\n",
    "                end = time.time()\n",
    "                print(\"Process completed in \" + str(round((end-start),2)) + \" seconds - Found a match in the sanction list with a score of \" + str(flag))\n",
    "                print(record)\n",
    "                # print(stitched_client_name, stitched_sanc_name)\n",
    "                return True\n",
    "    end = time.time()\n",
    "    print(\"Process completed in \" + str(round((end-start),2)) + \" seconds - with no match on the sanction list\")\n",
    "    return False    \n",
    "\n",
    "print(sanction_screening(test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process completed in 1.29 seconds - Found a match in the sanction list with a score of 100\n",
      "{'Unnamed: 0': 2040, 'name': 'son mun san', 'title': 'External Affairs Bureau Chief, General Bureau of Atomic Energy', 'dob': '23-Jan-51', 'pob': nan, 'nationality': nan, 'citizenship': nan, 'aliases': nan}\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "print(sanction_screening(test1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process completed in 8.22984504699707- Found a match in the sanction list with a score of 94\n",
      "{'Unnamed: 0': 1902, 'name': 'chaudhry aamir ali', 'title': nan, 'dob': '3-Aug-86', 'pob': nan, 'nationality': 'pakistan', 'citizenship': nan, 'aliases': \"['huzaifa']\"}\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\n",
    "test1 = {'name' : 'aamir ali chaudary'} # in the sanction list\n",
    "test2 = {'name' : 'lionel lew'}\n",
    "test3 = {'name' : 'murov evgen aleksyevic'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "test4 = 'murov evgeniy alekseyevich'\n",
    "test5 = 'murov evgen aleksyevic'\n",
    "abs(len(test4)-len(test5))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "print(sanction_screening(test3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Process completed in 9.4784677028656- Found a match in the sanction list with a score of 94\n",
      "{'Unnamed: 0': 2223, 'name': 'murov evgeniy alekseyevich', 'title': 'Director of the Federal Protective Service of the Russian Federation; Army General', 'dob': '18-Nov-45', 'pob': 'zvenigorod, moscow, russia', 'nationality': nan, 'citizenship': nan, 'aliases': nan}\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "print(len('murov evgen alekseyevic'))\n",
    "print(len('murov evgeniy alekseyevich'))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "23\n",
      "26\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "print(ER_name_matching(\"aamir ali chaudary\", \"amir ali chaundaary\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Name Matching"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}