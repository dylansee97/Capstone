{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jabezlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import dateutil.parser as parser\n",
    "from geopy.geocoders import Nominatim\n",
    "import pycountry\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pinyin\n",
    "\n",
    "from geopy.exc import GeocoderServiceError\n",
    "\n",
    "# Sentiment Analysis Packages\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name to be screened must be in English\n",
    "# Alias names can only handle Chinese characters , else return None\n",
    "def preprocess_df_to_dict(df):\n",
    "    def get_year(date):\n",
    "        try:\n",
    "            parser_obj = parser.parse(str(date))\n",
    "            return parser_obj.year\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_month(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).month\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def get_day(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).day\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "    \n",
    "    df_dict_list = df.to_dict('records')\n",
    "    cleaned_dict_list = []\n",
    "    for record in df_dict_list:\n",
    "        \n",
    "        alias = record['Alias name']\n",
    "        if alias is not None:\n",
    "            alias_is_english = isEnglish(alias)\n",
    "            if alias_is_english is False:\n",
    "                try:\n",
    "                    alias = pinyin.get(alias, format='strip', delimiter=' ')\n",
    "                except:\n",
    "                    alias = None\n",
    "        current_record = {\n",
    "            'name': record['Name to be screened'],\n",
    "            'alias' : alias,\n",
    "            'year_of_birth': get_year(record['Date of birth']),\n",
    "            'month_of_birth': get_month(record['Date of birth']),\n",
    "            'day_of_birth': get_day(record['Date of birth']),\n",
    "            'gender': record['Gender'],\n",
    "            'nationality': record['Nationality'],\n",
    "            ### delete these later on, for testing only###\n",
    "            'type_of_error': record['Type of variation (if any)'],\n",
    "            'actual_name': record['Actual name'],\n",
    "        }\n",
    "        cleaned_dict_list.append(current_record)\n",
    "    return cleaned_dict_list\n",
    "\n",
    "# Main Function\n",
    "def search_articles_on_individual(individual_dict, no_of_articles=10):\n",
    "    def generate_link(person_dict, attributes_used = ['name'], keywords=['crimes', 'sentenced']):\n",
    "        link_start = \"https://www.google.com/search?q=\"\n",
    "        link_end = \"&sxsrf=ALeKk01K1bOuJFHjy4HBARo1cRpUYakYPg:1629640327633&source=lnms&tbm=nws&sa=X&sqi=2&ved=2ahUKEwiu29um48TyAhWGqpUCHYuoAlcQ_AUoAnoECAEQBA&biw=1441&bih=718&dpr=2\" \n",
    "        link_query = \"\"\n",
    "\n",
    "        for attributes in attributes_used:\n",
    "            temp_attr = person_dict[attributes]\n",
    "            if temp_attr is not None:\n",
    "                temp_attr = str(temp_attr)\n",
    "                link_query += temp_attr.replace(' ', '+') + '+'       \n",
    "                \n",
    "        links = []\n",
    "        for keyword in keywords:\n",
    "            temp_search_link = link_start + link_query + keyword + link_end + \"&num=\" + str(no_of_articles)\n",
    "            links.append(temp_search_link)\n",
    "        return links\n",
    "    \n",
    "    def article_extraction(link):\n",
    "        article = Article(link)\n",
    "        article.download()\n",
    "        try:\n",
    "            article.parse()\n",
    "        except:\n",
    "            pass\n",
    "        return article.text\n",
    "\n",
    "    def parse(text):\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def find_names(tags):\n",
    "        names = []\n",
    "        for tag in tags:\n",
    "            if tag[1] == 'PERSON':\n",
    "                names.append(tag[0])\n",
    "        return names\n",
    "\n",
    "    def sentiment_analysis(text):\n",
    "        return vader.polarity_scores(text)\n",
    "    \n",
    "    def time_to_months(time):\n",
    "        if 'weeks' in time:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(time.split(' month')[0])\n",
    "\n",
    "    search_links = generate_link(individual_dict)\n",
    "    \n",
    "    unique_links_checker = []\n",
    "    \n",
    "    output = []\n",
    "    for x in search_links:\n",
    "        req = Request(x, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "        webpage = urlopen(req).read()\n",
    "\n",
    "        with requests.Session() as c:\n",
    "            soup = BeautifulSoup(webpage, 'html5lib')\n",
    "            #print(soup)\n",
    "            for item in soup.find_all('div', attrs = {'class': \"ZINbbc xpd O9g5cc uUPGi\"}):\n",
    "                current_dict = {}\n",
    "                raw_link = (item.find('a', href = True)['href'])\n",
    "                try:\n",
    "                    link = (raw_link.split(\"/url?q=\")[1]).split('&sa=U&')[0]\n",
    "                except IndexError as e1:\n",
    "                    continue\n",
    "                if link not in unique_links_checker and item:\n",
    "                    unique_links_checker.append(link)\n",
    "                    title = item.find('div',attrs = {'class': 'BNeawe vvjwJb AP7Wnd'})\n",
    "                    if title == None:\n",
    "                        continue\n",
    "                    title = title.get_text()\n",
    "                    description  = (item.find('div',attrs = {'class': 'BNeawe s3v9rd AP7Wnd'}).get_text())\n",
    "                    time = description.split(\" · \")[0]\n",
    "                    #print(description)\n",
    "                    descript = description.split(\" · \")[1]\n",
    "                    current_dict['title'] = title\n",
    "                    current_dict['time'] = time\n",
    "                    try:\n",
    "                        current_dict['year_of_birth'] = (date.today() - relativedelta(months = time_to_months(time))).year - individual_dict['year_of_birth']\n",
    "                    except TypeError as e1:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    except ValueError as e2:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    current_dict['description'] = descript\n",
    "                    current_dict['link'] = link\n",
    "                    current_dict['text'] = article_extraction(link)                    \n",
    "                    \n",
    "                    parsed_description = parse(description)\n",
    "                    names_in_description = find_names(parsed_description)\n",
    "                    \n",
    "                    parsed_text = parse(article_extraction(link))\n",
    "                    names_in_text = find_names(parsed_text)\n",
    "\n",
    "                    names_list = Counter(names_in_description + names_in_text)\n",
    "                    current_dict['names_list'] = names_list\n",
    "                    \n",
    "                    output.append(current_dict)\n",
    "                else:\n",
    "                    pass\n",
    "    return output\n",
    "\n",
    "def ER_name_matching(name1, name2):\n",
    "    def split_name_list(name):\n",
    "        name = name.lower()\n",
    "        output = name.split(\" \")\n",
    "        return output\n",
    "\n",
    "    def preprocess_name(names_dict, word):\n",
    "        for key, value in names_dict.items():\n",
    "            if word in value:\n",
    "                return key\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def stitch_name(list1):\n",
    "        output = ''\n",
    "        for x in range(len(list1)):\n",
    "            if x==0:\n",
    "                output += list1[x]\n",
    "            else:\n",
    "                output += ' ' + list1[x]\n",
    "        return output\n",
    "\n",
    "    def phonetic_comparison(list1, list2):\n",
    "        meta_list1 = []\n",
    "        meta_list2 = []\n",
    "        nysiis_list1 = []\n",
    "        nysiis_list2 = []\n",
    "        for name_1 in list1:\n",
    "            meta_list1.append(jellyfish.metaphone(name_1))\n",
    "            nysiis_list1.append(jellyfish.nysiis(name_1))\n",
    "        for name_2 in list2:\n",
    "            meta_list2.append(jellyfish.metaphone(name_2))\n",
    "            nysiis_list2.append(jellyfish.nysiis(name_2))\n",
    "        if (set(meta_list1) == set(meta_list2)) or (set(nysiis_list1) == set(nysiis_list2)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def excel_to_dict(excel_file):\n",
    "        excel_df = pd.read_excel(excel_file)\n",
    "        excel_df.value.apply(str)\n",
    "        before_transformation = dict(zip(excel_df.key, excel_df.value))\n",
    "        dictionary = {key: [val for val in value.split(',')] for key, value in before_transformation.items()}\n",
    "        return dictionary\n",
    "            \n",
    "    names_dict = excel_to_dict('names_dict.xlsx') \n",
    "    \n",
    "    # START #\n",
    "    ### Change this if needed ###\n",
    "    threshold = 89\n",
    "    #############################\n",
    "    \n",
    "    split_list_1 = split_name_list(name1)\n",
    "    split_list_2 = split_name_list(name2) \n",
    " \n",
    "    \n",
    "    for i in range(len(split_list_1)):\n",
    "        split_list_1[i] = preprocess_name(names_dict, split_list_1[i])        \n",
    "    for i in range(len(split_list_2)):\n",
    "        split_list_2[i] = preprocess_name(names_dict, split_list_2[i])\n",
    "    \n",
    "    stitched_name1 = stitch_name(split_list_1)\n",
    "    stitched_name2 = stitch_name(split_list_2)\n",
    "    \n",
    "    # 1st layer of testing: Token Sort Ratio with threshold\n",
    "    score1 = fuzz.token_sort_ratio(stitched_name1, stitched_name2)\n",
    "    if score1 >= threshold:\n",
    "        # score_list.append(score1)\n",
    "        return score1\n",
    "        # do something\n",
    "# 4) 2nd layer of testing - Metaphone and NYSIIS phonetic encoding - DONE\n",
    "    else: \n",
    "        matched_phonetic = phonetic_comparison(split_list_1, split_list_2)\n",
    "        if matched_phonetic:\n",
    "            return threshold # assumption that phonetic match will give threshold score\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        return score1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# hlpr func: get country by cities, states name\n",
    "def get_country(gpe):\n",
    "    geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "    location = geolocator.geocode(gpe)\n",
    "    if location:\n",
    "        loc_lst = location.address.split(',')\n",
    "        return loc_lst[-1]\n",
    "    return None\n",
    "\n",
    "# hlpr func: return a list of countries names\n",
    "def countries():\n",
    "    return list(map(lambda x: x.name, list(pycountry.countries)))\n",
    "\n",
    "# hlpr func: return True if name countains country name\n",
    "def contain_country(word, ctry_lst):\n",
    "    for ctry in ctry_lst:\n",
    "        if ctry.lower() in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# hlpr func: extract entities with tag 'GPE', 'ORG', 'NORP'\n",
    "def search_target_ent(tags):\n",
    "    country_lst = countries()\n",
    "    tag_lst = []\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i][1] == 'GPE' or tags[i][1] == 'ORG' or tags[i][1] == 'NORP':\n",
    "            if contain_country(tags[i][0], country_lst):\n",
    "                tag_lst.append(tags[i])\n",
    "    return tag_lst\n",
    "\n",
    "# hlpr func: return the odd of the person's nationality in the article is nat\n",
    "def calc_odd_nationality(nat,lst):\n",
    "    try:\n",
    "        result = []\n",
    "        for tag in lst:\n",
    "            if tag[0] is not None and nat is not None:\n",
    "                if nat.lower() in tag[0].lower():\n",
    "                    result.append(tag)\n",
    "                    continue\n",
    "            try:\n",
    "                if tag[1] == 'GPE' and (get_country(tag[0]) is not None and nat is not None):\n",
    "                    if get_country(tag[0]).lower() == nat.lower():\n",
    "                        result.append(tag)\n",
    "            except GeocoderServiceError as e:\n",
    "                pass\n",
    "        prob = 1 if ((len(lst) - len(result)) == 0 and len(result) > 0) else (len(result) / (len(lst) - len(result)))\n",
    "        prob = 1 if prob > 1 else prob\n",
    "        return prob\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "\n",
    "# hlpr func: return True if name fuzzy matching score > 80\n",
    "def is_target(name, article_name):\n",
    "    return fuzz.partial_ratio(name, article_name) > 80\n",
    "\n",
    "# the main function for nationality matching\n",
    "# return odd if target tags found else return -1\n",
    "def nationality_matching(tags, nationality, person):\n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "            if tags[i][1] == 'PERSON':\n",
    "                \n",
    "                # check if is target\n",
    "                if is_target(person, tags[i][0]):\n",
    "                    search = search_target_ent(tags)\n",
    "                \n",
    "                    if len(search) != 0:\n",
    "                        return calc_odd_nationality(nationality, search)\n",
    "        return -1\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "\n",
    "# hlpr func: parse text to tags\n",
    "def parse(text):\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "# hlpr func: return True if token is a name and subject\n",
    "def is_name_subj(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ == 'nsubjpass')  and token.pos_ == 'PROPN'\n",
    "\n",
    "def is_part_of_name(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ =='compound' or token.dep_ == 'nsubjpass') \\\n",
    "        and token.pos_ == 'PROPN'\n",
    "\n",
    "# hlpr func: return True if the token is a determiner: his, her, hers\n",
    "def is_det(token):\n",
    "    return token.pos_ == 'DET' and (token.dep_ == 'poss' or token.dep_ == 'attr')\n",
    "\n",
    "# hlpr func: return True if the token is a pronoun: he, she, herself, himself\n",
    "def is_pron(token):\n",
    "    return token.pos_ == 'PRON' and \\\n",
    "        (token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' or token.dep_ == 'pobj' or token.dep_ == 'dobj')\n",
    "\n",
    "# hlpr func: return True if the gender noun is referring to target person\n",
    "def refer_target(gender, noun, name, text):\n",
    "    m = ['man', 'boy', 'guy']\n",
    "    f = ['woman', 'lady', 'girl']\n",
    "    \n",
    "    if is_target(name, text):\n",
    "        return (gender == 'male' and noun in m) or (gender == 'female' and noun in f)\n",
    "    return 0\n",
    "\n",
    "# hlpr func: return True if gender noun is follwed by 'is, was, as or comma'\n",
    "def gender_noun(t1, t2):\n",
    "    gender_nouns = ['man', 'boy', 'guy', 'woman', 'lady', 'girl']\n",
    "    verbs = ['was', 'is', 'as', ',']\n",
    "    return (t1 in gender_nouns) and (t2 in verbs)\n",
    "\n",
    "# hlpr func: return the probability of the gender in article to the true gender\n",
    "def calc_prob_gender(pron_lst, gender):\n",
    "    male_pron = ['he', 'his', 'himself', 'him']\n",
    "    female_pron = ['she', 'her', 'herself', 'hers']\n",
    "    n_target = 0\n",
    "    gdr_pron = []\n",
    "    \n",
    "    if gender.lower() == 'male':\n",
    "        gdr_pron = male_pron\n",
    "    else:\n",
    "        gdr_pron = female_pron\n",
    "        \n",
    "    for pron in pron_lst:\n",
    "        if pron in gdr_pron:\n",
    "            n_target += 1\n",
    "    return n_target / len(pron_lst) if len(pron_lst) else 0\n",
    "\n",
    "# the main function in gender matching\n",
    "def gender_matching(text, gender, name):\n",
    "    try:\n",
    "        pron_lst = ['he', 'his', 'himself', 'him', 'she', 'her', 'herself', 'hers']\n",
    "        name_str = ''\n",
    "        target_name = name.replace(\" \", \"\")\n",
    "        target_found = False\n",
    "        res_lst = []\n",
    "        \n",
    "        # text tagging\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(doc):\n",
    "\n",
    "            # catch text like '...woman is/was/as/, xxx...'\n",
    "            if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                    return (1)\n",
    "\n",
    "            # search for target name of subject form\n",
    "            if is_name_subj(doc[i]):\n",
    "                end_name = i\n",
    "                start_name = i\n",
    "                while is_part_of_name(doc[start_name]):\n",
    "                    start_name -= 1\n",
    "                start_name += 1\n",
    "                while start_name <= end_name:\n",
    "                    name_str += doc[start_name].text\n",
    "                    start_name += 1\n",
    "\n",
    "            if is_target(name_str, target_name):\n",
    "                target_found = True\n",
    "            else:\n",
    "                name_str = ''\n",
    "                target_found = False\n",
    "          \n",
    "            # if target name is found, search for pronouns, break if another name is found\n",
    "            while target_found:\n",
    "                i+=1\n",
    "                if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                    if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                        return (1)\n",
    "                if is_name_subj(doc[i]):\n",
    "                    target_found = False\n",
    "                    name_str = ''\n",
    "                    break\n",
    "                if is_det(doc[i]) or is_pron(doc[i]):\n",
    "                    if (doc[i].text).lower() in pron_lst:\n",
    "                        res_lst.append((doc[i].text).lower())\n",
    "                        break\n",
    "\n",
    "            i += 1\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return calc_prob_gender(res_lst, gender)\n",
    "\n",
    "useless_dates = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','yesterday','today']\n",
    "\n",
    "#index is index of person\n",
    "def forward_searcher(index,tags):\n",
    "    for i in range(index,len(tags)):\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "    return [None,None]\n",
    "\n",
    "def backward_searcher(index,tags):\n",
    "    i = index\n",
    "    while i >= 0:\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "        else:\n",
    "            i -=1\n",
    "\n",
    "def detect_age(age,lst):\n",
    "    try:\n",
    "        if lst[1] is not None and lst[2] is not None:\n",
    "            date1 = lst[1][0]\n",
    "            date2 = lst[2][0]\n",
    "            if (str(age) in date1) or (str(age) in date2):\n",
    "                return True\n",
    "        else:\n",
    "\n",
    "            if lst[1] == None:\n",
    "                if str(age) in lst[2][0]:\n",
    "                    return True\n",
    "\n",
    "            if lst[2] == None:\n",
    "                if str(age) in lst[1][0]:\n",
    "                    return True\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def confirm_age(lst,age,threshold):\n",
    "    iterating_lst = []\n",
    "    plus = 1\n",
    "    minus = -1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+plus)\n",
    "        plus += 1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+minus)\n",
    "        minus -=1 \n",
    "    iterating_lst.append(age)\n",
    "    \n",
    "    for j in iterating_lst:\n",
    "        if str(j) in lst[1][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def age_matching(name_dict,tags,age):\n",
    "    '''\n",
    "    tags: parse(text)\n",
    "    age: desired age to check\n",
    "    '''\n",
    "    for tag in tags:\n",
    "        #if tag[1] == 'DATE':\n",
    "            #print(tag)\n",
    "        if str(age) in tag[0] or str(age+1) in tag[0] or str(age-1) in tag[0]:\n",
    "            return 1\n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "\n",
    "            \n",
    "            if tags[i][1] == 'PERSON':\n",
    "                if tags[i][0] in name_dict:\n",
    "\n",
    "                    forward_age = forward_searcher(i,tags)\n",
    "                    backwards_age = backward_searcher(i,tags)\n",
    "                    new_list = [tags[i],forward_age,backwards_age]\n",
    "                    #new_list = [tags[i-1],tags[i],tags[i+1]]\n",
    "                    #print(new_list)\n",
    "\n",
    "                    if detect_age(age,new_list) and tags[i][0] in name_list:\n",
    "\n",
    "                        #print(new_list)\n",
    "                        #result += new_list\n",
    "\n",
    "                        if str(age) in new_list[1][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i], new_list[1]])\n",
    "                            return(confirm_age([tags[i],new_list[1]],age,3))\n",
    "\n",
    "\n",
    "                        elif str(age) in new_list[2][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i],new_list[2]])\n",
    "                            return(confirm_age([tags[i],new_list[2]],age,3))\n",
    "                        \n",
    "        return 0\n",
    "    except IndexError as e:\n",
    "        pass \n",
    "    \n",
    "def entity_recognition_scoring(input_info, list_of_article_dicts):\n",
    "    output = []\n",
    "    input_name = input_info['name']\n",
    "\n",
    "    for article in list_of_article_dicts:\n",
    "        article_names_list = article['names_list'].most_common() \n",
    "        matched = False\n",
    "        \n",
    "        for each_name, each_count in article_names_list: ## as of now checking all names within the article, should we limit to e.g. top 3/5?\n",
    "            if len(each_name.split()) == 1 and each_name in input_name:\n",
    "                score = 100 ## if surname matches, default match score 100 \n",
    "            else: \n",
    "                try: \n",
    "                    score = ER_name_matching(input_name, each_name)\n",
    "                except ValueError as e:\n",
    "                    pass\n",
    "            if score is not None:\n",
    "                matched = True\n",
    "            if matched:\n",
    "                break\n",
    "     \n",
    "        if matched:\n",
    "            # keep article\n",
    "            article['name_matching_score'] = score\n",
    "\n",
    "            # nationality matching score\n",
    "            article ['nationality_matching_score'] = nationality_matching(parse(article['text']), input_info['nationality'], input_info['name'])\n",
    "\n",
    "            # gender matching score\n",
    "            article ['gender_matching_score'] =gender_matching(article['text'], input_info['gender'], input_info['name'])\n",
    "            \n",
    "            article['age_matching_score'] = age_matching(article['names_list'],parse(article['text']),article['year_of_birth'])\n",
    "            #print(article['year_of_birth'])\n",
    "\n",
    "            output.append(article)\n",
    "    return output\n",
    "\n",
    "def entity_recognition_scoring(input_info, list_of_article_dicts):\n",
    "    output = []\n",
    "    input_name = input_info['name']\n",
    "\n",
    "    for article in list_of_article_dicts:\n",
    "        article_names_list = article['names_list'].most_common() \n",
    "        matched = False\n",
    "        \n",
    "        for each_name, each_count in article_names_list: ## as of now checking all names within the article, should we limit to e.g. top 3/5?\n",
    "            if len(each_name.split()) == 1 and each_name in input_name:\n",
    "                score = 100 ## if surname matches, default match score 100 \n",
    "            else: \n",
    "                try: \n",
    "                    score = ER_name_matching(input_name, each_name)\n",
    "                except ValueError as e:\n",
    "                    pass\n",
    "            if score is not None:\n",
    "                matched = True\n",
    "            if matched:\n",
    "                break\n",
    "     \n",
    "        if matched:\n",
    "            # keep article\n",
    "            article['name_matching_score'] = score\n",
    "\n",
    "            # nationality matching score\n",
    "            article ['nationality_matching_score'] = nationality_matching(parse(article['text']), input_info['nationality'], input_info['name'])\n",
    "\n",
    "            # gender matching score\n",
    "            article ['gender_matching_score'] =gender_matching(article['text'], input_info['gender'], input_info['name'])\n",
    "            \n",
    "            article['age_matching_score'] = age_matching(article['names_list'],parse(article['text']),article['year_of_birth'])\n",
    "            #print(article['year_of_birth'])\n",
    "\n",
    "            output.append(article)\n",
    "    return output\n",
    "\n",
    "# Sentiment Analysis\n",
    "def sentiment_model(name_matched):\n",
    "\n",
    "    # Loading Model\n",
    "    reconstructed_model = keras.models.load_model(\"LSTM_GLOVE\")\n",
    "\n",
    "    url = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
    "    (?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([\n",
    "      ^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    def clean_data(temp):\n",
    "        temp = temp.map(lambda x:str(x).lower()) \n",
    "        # removing emails\n",
    "        temp = temp.map(lambda x:re.sub(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\", \"\", x)) \n",
    "        # removing url\n",
    "        temp = temp.map(lambda x:re.sub(url, \"\", x)) \n",
    "        # removing numbers\n",
    "        temp = temp.map(lambda x:re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \"\", x)) \n",
    "        # removing white space\n",
    "        temp = temp.map(lambda x:re.sub(r'^\\s*|\\s\\s*', ' ', x).strip()) \n",
    "        # removing punctuations\n",
    "        temp = temp.map(lambda x:''.join([c for c in x if c not in string.punctuation])) \n",
    "        # removing special characters\n",
    "        temp = temp.map(lambda x:re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', x)) \n",
    "        # unicode\n",
    "        temp = temp.map(lambda x:unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')) \n",
    "        # tokenising text for cleaning\n",
    "        temp = temp.map(lambda x:tokenizer.tokenize(x)) \n",
    "        # removing stop words\n",
    "        temp = temp.map(lambda x:[i for i in x if i not in stopwords.words('english')]) \n",
    "        temp = temp.map(lambda x:' '.join(x))\n",
    "        return temp\n",
    "\n",
    "    name_matched['body'] = name_matched['text']\n",
    "    name_matched.text = clean_data(name_matched.text)\n",
    "\n",
    "    # Data Preprocessing for model ingestion\n",
    "    maxlen = 50\n",
    "    embedding_dim = 100\n",
    "\n",
    "    X = name_matched.text.values\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(name_matched.text.values)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    test_input = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
    "\n",
    "    # Predicting output\n",
    "    test = reconstructed_model.predict(test_input)\n",
    "    test_classes = np.argmax(test,axis=1)\n",
    "    name_matched['prediction'] = test_classes\n",
    "\n",
    "    def predicted_classes(df):\n",
    "        val = ''  \n",
    "        if df['prediction'] == 1:\n",
    "            val = 'negative'\n",
    "        elif df['prediction'] == 0:\n",
    "            val = 'neutral'\n",
    "        else:\n",
    "            val = 'positive'\n",
    "\n",
    "        return val\n",
    "\n",
    "    name_matched['prediction'] = name_matched.apply(predicted_classes,axis=1)\n",
    "    name_matched = name_matched[['title', 'time', 'year_of_birth', 'description', 'link', 'body', \n",
    "                                   'names_list', 'name_matching_score',\n",
    "                                   'nationality_matching_score', 'gender_matching_score',\n",
    "                                   'age_matching_score', 'prediction']]\n",
    "    \n",
    "    return name_matched\n",
    "\n",
    "def demo():\n",
    "    # Web Scraping\n",
    "    df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    df_dict = preprocess_df_to_dict(df)\n",
    "    test_record_1 = df_dict[0]\n",
    "    print('Test input: \\n')\n",
    "    print('Name: ' + str(test_record_1['name']))\n",
    "    print('Alias: ' + str(test_record_1['alias']))\n",
    "    print('Year of Birth: ' + str(test_record_1['year_of_birth']))\n",
    "    print('Month of Birth: ' + str(test_record_1['month_of_birth']))\n",
    "    print('Day of Birth: ' + str(test_record_1['day_of_birth']))\n",
    "    print('Gender: ' + str(test_record_1['gender']))\n",
    "    print('Nationality: ' + str(test_record_1['nationality']))\n",
    "    print('Actual Name: ' + str(test_record_1['actual_name']))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    test_query = search_articles_on_individual(test_record_1, 10)\n",
    "\n",
    "    # Name Matching\n",
    "    name_matched = entity_recognition_scoring(test_record_1, test_query)\n",
    "    name_matched = pd.DataFrame(name_matched)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    output = sentiment_model(name_matched)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \n",
      "\n",
      "Name: Loh Hyun Jin Nelly\n",
      "Alias: None\n",
      "Year of Birth: 1997\n",
      "Month of Birth: None\n",
      "Day of Birth: None\n",
      "Gender: Male\n",
      "Nationality: Singapore\n",
      "Actual Name: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Web Scraping\n",
    "df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df_dict = preprocess_df_to_dict(df)\n",
    "test_record_1 = df_dict[7]\n",
    "print('Test input: \\n')\n",
    "print('Name: ' + str(test_record_1['actual_name']))\n",
    "print('Alias: ' + str(test_record_1['alias']))\n",
    "print('Year of Birth: ' + str(test_record_1['year_of_birth']))\n",
    "print('Month of Birth: ' + str(test_record_1['month_of_birth']))\n",
    "print('Day of Birth: ' + str(test_record_1['day_of_birth']))\n",
    "print('Gender: ' + str(test_record_1['gender']))\n",
    "print('Nationality: ' + str(test_record_1['nationality']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test_query = search_articles_on_individual(test_record_1, 10)\n",
    "\n",
    "# Name Matching\n",
    "name_matched = entity_recognition_scoring(test_record_1, test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \n",
      "\n",
      "Name: Tan Wee Beng\n",
      "Alias: None\n",
      "Year of Birth: 1977\n",
      "Month of Birth: None\n",
      "Day of Birth: None\n",
      "Gender: Male\n",
      "Nationality: Singapore\n",
      "Actual Name: Tan Wee Beng\n",
      "\n",
      "\n",
      "CPU times: user 32.3 s, sys: 1.02 s, total: 33.3 s\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>time</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>body</th>\n",
       "      <th>names_list</th>\n",
       "      <th>name_matching_score</th>\n",
       "      <th>nationality_matching_score</th>\n",
       "      <th>gender_matching_score</th>\n",
       "      <th>age_matching_score</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man on FBI's most wanted list pleads guilty in...</td>\n",
       "      <td>8 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>Tan Wee Beng, 44, pleaded guilty to seven char...</td>\n",
       "      <td>https://www.channelnewsasia.com/singapore/tan-...</td>\n",
       "      <td>SINGAPORE: A Singaporean man who is on the Uni...</td>\n",
       "      <td>{'Tan Wee Beng': 2, 'Tan': 8, 'Wee Tiong': 5, ...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singaporean on FBI's most wanted list charged ...</td>\n",
       "      <td>15 months ago</td>\n",
       "      <td>43</td>\n",
       "      <td>Tan Wee Beng is the managing director of Wee T...</td>\n",
       "      <td>https://www.channelnewsasia.com/singapore/sing...</td>\n",
       "      <td>SINGAPORE: A 43-year-old Singaporean managing ...</td>\n",
       "      <td>{'Tan Wee Beng': 2, 'Wee Tiong': 3, 'Tan': 5, ...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Singapore trader denies laundering millions fo...</td>\n",
       "      <td>35 months ago</td>\n",
       "      <td>41</td>\n",
       "      <td>Tan Wee Beng told the BBC that he had only lea...</td>\n",
       "      <td>https://www.bbc.co.uk/news/world-asia-45987533</td>\n",
       "      <td>The US Treasury has now placed sanctions on Mr...</td>\n",
       "      <td>{'Tan Wee Beng': 1, 'Mr Tan': 1, 'Wee Tiong': 1}</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Singaporean MD on FBI most wanted list charged...</td>\n",
       "      <td>15 months ago</td>\n",
       "      <td>43</td>\n",
       "      <td>Tan Wee Beng, a 43-year old Singaporean managi...</td>\n",
       "      <td>https://www.manifoldtimes.com/news/singaporean...</td>\n",
       "      <td>Tan Wee Beng, a 43-year old Singaporean managi...</td>\n",
       "      <td>{'Tan Wee Beng': 4, 'Wee Tiong': 3, 'Tan': 4, ...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singaporean accused of N. Korea dealings to ta...</td>\n",
       "      <td>35 months ago</td>\n",
       "      <td>41</td>\n",
       "      <td>SINGAPORE (AP) -- A Singaporean businessman fa...</td>\n",
       "      <td>https://mainichi.jp/english/articles/20181031/...</td>\n",
       "      <td>This wanted poster released by FBI shows Singa...</td>\n",
       "      <td>{'Wee Tiong': 6, 'Tan Wee Beng': 2, 'Singapore...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FBI Seeks Trader Accused of Violating North Ko...</td>\n",
       "      <td>35 months ago</td>\n",
       "      <td>41</td>\n",
       "      <td>Beng (aliases \"WB\", Wee Beng Tan, Marcus Tan, ...</td>\n",
       "      <td>https://www.maritime-executive.com/article/fbi...</td>\n",
       "      <td>FBI Seeks Trader Accused of Violating North Ko...</td>\n",
       "      <td>{'Beng': 6, 'Wee Beng Tan': 2, 'Marcus Tan': 2...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Top stories from The Straits Times on Friday, ...</td>\n",
       "      <td>35 months ago</td>\n",
       "      <td>41</td>\n",
       "      <td>US imposes North Korea-related sanctions on Si...</td>\n",
       "      <td>https://www.straitstimes.com/singapore/top-sto...</td>\n",
       "      <td>US imposes North Korea-related sanctions on Si...</td>\n",
       "      <td>{'Tan Wee Beng': 2, 'Steven Chong': 2, 'Davind...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>North Korea Built an Alternative Financial Sys...</td>\n",
       "      <td>33 months ago</td>\n",
       "      <td>42</td>\n",
       "      <td>The co-owner, Tan Wee Beng, 41 years old, rece...</td>\n",
       "      <td>https://www.wsj.com/articles/north-korea-built...</td>\n",
       "      <td></td>\n",
       "      <td>{'Tan Wee Beng': 1}</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>S'pore businessman wanted by FBI pleads guilty...</td>\n",
       "      <td>3 hours ago</td>\n",
       "      <td>0</td>\n",
       "      <td>Tan Wee Beng, 44, was convicted of seven count...</td>\n",
       "      <td>https://www.straitstimes.com/singapore/courts-...</td>\n",
       "      <td>SINGAPORE - A Singaporean businessman, on the ...</td>\n",
       "      <td>{'Tan Wee Beng': 2, 'Tan': 12, 'Wee Tiong': 4,...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Singapore commodity broker wanted by FBI charg...</td>\n",
       "      <td>15 months ago</td>\n",
       "      <td>43</td>\n",
       "      <td>Tan Wee Beng, managing director of trading com...</td>\n",
       "      <td>https://www.malaymail.com/news/singapore/2020/...</td>\n",
       "      <td>The North Korea flag flutters next to concerti...</td>\n",
       "      <td>{'Tan Wee Beng': 2, 'Wee Tiong': 3, 'Tan': 3}</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4 scooter riders jailed for assaulting Argenti...</td>\n",
       "      <td>41 months ago</td>\n",
       "      <td>41</td>\n",
       "      <td>The four — Thomas Leong Sin Kwang, 37, Tay Woe...</td>\n",
       "      <td>https://stomp.straitstimes.com/singapore-seen/...</td>\n",
       "      <td>Four motor scooter riders were sentenced to ja...</td>\n",
       "      <td>{'Thomas Leong Sin Kwang': 2, 'Tay Woei Chain'...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Food stall attendant who stomped on man's face...</td>\n",
       "      <td>31 months ago</td>\n",
       "      <td>42</td>\n",
       "      <td>The older man was later taken to Tan Tock Seng...</td>\n",
       "      <td>https://stomp.straitstimes.com/singapore-seen/...</td>\n",
       "      <td>Shaffiq Alkhatib\\n\\nThe Straits Times\\n\\nFeb 1...</td>\n",
       "      <td>{'Wee': 7, 'Shaffiq Alkhatib': 1, 'Wee Boon': ...</td>\n",
       "      <td>100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title           time  \\\n",
       "0   Man on FBI's most wanted list pleads guilty in...    8 hours ago   \n",
       "1   Singaporean on FBI's most wanted list charged ...  15 months ago   \n",
       "2   Singapore trader denies laundering millions fo...  35 months ago   \n",
       "3   Singaporean MD on FBI most wanted list charged...  15 months ago   \n",
       "4   Singaporean accused of N. Korea dealings to ta...  35 months ago   \n",
       "5   FBI Seeks Trader Accused of Violating North Ko...  35 months ago   \n",
       "6   Top stories from The Straits Times on Friday, ...  35 months ago   \n",
       "7   North Korea Built an Alternative Financial Sys...  33 months ago   \n",
       "8   S'pore businessman wanted by FBI pleads guilty...    3 hours ago   \n",
       "9   Singapore commodity broker wanted by FBI charg...  15 months ago   \n",
       "10  4 scooter riders jailed for assaulting Argenti...  41 months ago   \n",
       "11  Food stall attendant who stomped on man's face...  31 months ago   \n",
       "\n",
       "    year_of_birth                                        description  \\\n",
       "0               0  Tan Wee Beng, 44, pleaded guilty to seven char...   \n",
       "1              43  Tan Wee Beng is the managing director of Wee T...   \n",
       "2              41  Tan Wee Beng told the BBC that he had only lea...   \n",
       "3              43  Tan Wee Beng, a 43-year old Singaporean managi...   \n",
       "4              41  SINGAPORE (AP) -- A Singaporean businessman fa...   \n",
       "5              41  Beng (aliases \"WB\", Wee Beng Tan, Marcus Tan, ...   \n",
       "6              41  US imposes North Korea-related sanctions on Si...   \n",
       "7              42  The co-owner, Tan Wee Beng, 41 years old, rece...   \n",
       "8               0  Tan Wee Beng, 44, was convicted of seven count...   \n",
       "9              43  Tan Wee Beng, managing director of trading com...   \n",
       "10             41  The four — Thomas Leong Sin Kwang, 37, Tay Woe...   \n",
       "11             42  The older man was later taken to Tan Tock Seng...   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://www.channelnewsasia.com/singapore/tan-...   \n",
       "1   https://www.channelnewsasia.com/singapore/sing...   \n",
       "2      https://www.bbc.co.uk/news/world-asia-45987533   \n",
       "3   https://www.manifoldtimes.com/news/singaporean...   \n",
       "4   https://mainichi.jp/english/articles/20181031/...   \n",
       "5   https://www.maritime-executive.com/article/fbi...   \n",
       "6   https://www.straitstimes.com/singapore/top-sto...   \n",
       "7   https://www.wsj.com/articles/north-korea-built...   \n",
       "8   https://www.straitstimes.com/singapore/courts-...   \n",
       "9   https://www.malaymail.com/news/singapore/2020/...   \n",
       "10  https://stomp.straitstimes.com/singapore-seen/...   \n",
       "11  https://stomp.straitstimes.com/singapore-seen/...   \n",
       "\n",
       "                                                 body  \\\n",
       "0   SINGAPORE: A Singaporean man who is on the Uni...   \n",
       "1   SINGAPORE: A 43-year-old Singaporean managing ...   \n",
       "2   The US Treasury has now placed sanctions on Mr...   \n",
       "3   Tan Wee Beng, a 43-year old Singaporean managi...   \n",
       "4   This wanted poster released by FBI shows Singa...   \n",
       "5   FBI Seeks Trader Accused of Violating North Ko...   \n",
       "6   US imposes North Korea-related sanctions on Si...   \n",
       "7                                                       \n",
       "8   SINGAPORE - A Singaporean businessman, on the ...   \n",
       "9   The North Korea flag flutters next to concerti...   \n",
       "10  Four motor scooter riders were sentenced to ja...   \n",
       "11  Shaffiq Alkhatib\\n\\nThe Straits Times\\n\\nFeb 1...   \n",
       "\n",
       "                                           names_list  name_matching_score  \\\n",
       "0   {'Tan Wee Beng': 2, 'Tan': 8, 'Wee Tiong': 5, ...                  100   \n",
       "1   {'Tan Wee Beng': 2, 'Wee Tiong': 3, 'Tan': 5, ...                  100   \n",
       "2    {'Tan Wee Beng': 1, 'Mr Tan': 1, 'Wee Tiong': 1}                  100   \n",
       "3   {'Tan Wee Beng': 4, 'Wee Tiong': 3, 'Tan': 4, ...                  100   \n",
       "4   {'Wee Tiong': 6, 'Tan Wee Beng': 2, 'Singapore...                  100   \n",
       "5   {'Beng': 6, 'Wee Beng Tan': 2, 'Marcus Tan': 2...                  100   \n",
       "6   {'Tan Wee Beng': 2, 'Steven Chong': 2, 'Davind...                  100   \n",
       "7                                 {'Tan Wee Beng': 1}                  100   \n",
       "8   {'Tan Wee Beng': 2, 'Tan': 12, 'Wee Tiong': 4,...                  100   \n",
       "9       {'Tan Wee Beng': 2, 'Wee Tiong': 3, 'Tan': 3}                  100   \n",
       "10  {'Thomas Leong Sin Kwang': 2, 'Tay Woei Chain'...                  100   \n",
       "11  {'Wee': 7, 'Shaffiq Alkhatib': 1, 'Wee Boon': ...                  100   \n",
       "\n",
       "    nationality_matching_score  gender_matching_score  age_matching_score  \\\n",
       "0                         1.00               1.000000                   1   \n",
       "1                         1.00               0.000000                   0   \n",
       "2                        -1.00               0.000000                   0   \n",
       "3                         1.00               1.000000                   0   \n",
       "4                         1.00               1.000000                   1   \n",
       "5                         0.75               1.000000                   0   \n",
       "6                         1.00               0.000000                   1   \n",
       "7                        -1.00               0.000000                   0   \n",
       "8                         1.00               1.000000                   1   \n",
       "9                         1.00               0.000000                   0   \n",
       "10                        0.00               0.777778                   1   \n",
       "11                        1.00               1.000000                   0   \n",
       "\n",
       "   prediction  \n",
       "0    negative  \n",
       "1    positive  \n",
       "2    negative  \n",
       "3     neutral  \n",
       "4    positive  \n",
       "5    positive  \n",
       "6    negative  \n",
       "7    negative  \n",
       "8    positive  \n",
       "9    positive  \n",
       "10   positive  \n",
       "11   positive  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "demo()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a41f48461941a5aefc82610e94371dafd9500cffa630c3ca9566e477af78c3ed"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
