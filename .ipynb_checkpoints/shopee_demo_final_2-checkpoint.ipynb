{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jabezlee/opt/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jabezlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "import dateutil.parser as parser\n",
    "from geopy.geocoders import Nominatim\n",
    "import pycountry\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pinyin\n",
    "import sys\n",
    "\n",
    "from geopy.exc import GeocoderServiceError\n",
    "\n",
    "# Sentiment Analysis Packages\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name to be screened must be in English\n",
    "# Alias names can only handle Chinese characters , else return None\n",
    "def preprocess_df_to_dict(df):\n",
    "    def get_year(date):\n",
    "        try:\n",
    "            parser_obj = parser.parse(str(date))\n",
    "            return parser_obj.year\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_month(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).month\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    def get_day(date):\n",
    "        if len(str(date))>4:\n",
    "            try:\n",
    "                return parser.parse(str(date)).day\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def isEnglish(s):\n",
    "        try:\n",
    "            s.encode(encoding='utf-8').decode('ascii')\n",
    "        except UnicodeDecodeError:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "    \n",
    "    df_dict_list = df.to_dict('records')\n",
    "    cleaned_dict_list = []\n",
    "    for record in df_dict_list:\n",
    "        \n",
    "        alias = record['Alias name']\n",
    "        if alias is not None:\n",
    "            alias_is_english = isEnglish(alias)\n",
    "            if alias_is_english is False:\n",
    "                try:\n",
    "                    alias = pinyin.get(alias, format='strip', delimiter=' ')\n",
    "                except:\n",
    "                    alias = None\n",
    "        current_record = {\n",
    "            'name': record['Name to be screened'],\n",
    "            'alias' : alias,\n",
    "            'year_of_birth': get_year(record['Date of birth']),\n",
    "            'month_of_birth': get_month(record['Date of birth']),\n",
    "            'day_of_birth': get_day(record['Date of birth']),\n",
    "            'gender': record['Gender'],\n",
    "            'nationality': record['Nationality'],\n",
    "            ### delete these later on, for testing only###\n",
    "            'type_of_error': record['Type of variation (if any)'],\n",
    "            'actual_name': record['Actual name'],\n",
    "        }\n",
    "        cleaned_dict_list.append(current_record)\n",
    "    return cleaned_dict_list\n",
    "\n",
    "def ER_name_matching(name1, name2):\n",
    "    def split_name_list(name):\n",
    "        name = name.lower()\n",
    "        output = name.split(\" \")\n",
    "        return output\n",
    "\n",
    "    def preprocess_name(names_dict, word):\n",
    "        for key, value in names_dict.items():\n",
    "            if word in value:\n",
    "                return key\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "    def stitch_name(list1):\n",
    "        output = ''\n",
    "        for x in range(len(list1)):\n",
    "            if x==0:\n",
    "                output += list1[x]\n",
    "            else:\n",
    "                output += ' ' + list1[x]\n",
    "        return output\n",
    "\n",
    "    def phonetic_comparison(list1, list2):\n",
    "        meta_list1 = []\n",
    "        meta_list2 = []\n",
    "        nysiis_list1 = []\n",
    "        nysiis_list2 = []\n",
    "        for name_1 in list1:\n",
    "            meta_list1.append(jellyfish.metaphone(name_1))\n",
    "            nysiis_list1.append(jellyfish.nysiis(name_1))\n",
    "        for name_2 in list2:\n",
    "            meta_list2.append(jellyfish.metaphone(name_2))\n",
    "            nysiis_list2.append(jellyfish.nysiis(name_2))\n",
    "        if (set(meta_list1) == set(meta_list2)) or (set(nysiis_list1) == set(nysiis_list2)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def excel_to_dict(excel_file):\n",
    "        excel_df = pd.read_excel(excel_file)\n",
    "        excel_df.value.apply(str)\n",
    "        before_transformation = dict(zip(excel_df.key, excel_df.value))\n",
    "        dictionary = {key: [val for val in value.split(',')] for key, value in before_transformation.items()}\n",
    "        return dictionary\n",
    "            \n",
    "    names_dict = excel_to_dict('names_dict.xlsx') \n",
    "    \n",
    "    # START #\n",
    "    ### Change this if needed ###\n",
    "    threshold = 89\n",
    "    #############################\n",
    "    \n",
    "    split_list_1 = split_name_list(name1)\n",
    "    split_list_2 = split_name_list(name2) \n",
    " \n",
    "    \n",
    "    for i in range(len(split_list_1)):\n",
    "        split_list_1[i] = preprocess_name(names_dict, split_list_1[i])        \n",
    "    for i in range(len(split_list_2)):\n",
    "        split_list_2[i] = preprocess_name(names_dict, split_list_2[i])\n",
    "    \n",
    "    stitched_name1 = stitch_name(split_list_1)\n",
    "    stitched_name2 = stitch_name(split_list_2)\n",
    "    \n",
    "    # 1st layer of testing: Token Sort Ratio with threshold\n",
    "    score1 = fuzz.token_sort_ratio(stitched_name1, stitched_name2)\n",
    "    if score1 >= threshold:\n",
    "        # score_list.append(score1)\n",
    "        return score1\n",
    "        # do something\n",
    "# 4) 2nd layer of testing - Metaphone and NYSIIS phonetic encoding - DONE\n",
    "    else: \n",
    "        matched_phonetic = phonetic_comparison(split_list_1, split_list_2)\n",
    "        if matched_phonetic:\n",
    "            return threshold # assumption that phonetic match will give threshold score\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        return score1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# hlpr func: get country by cities, states name\n",
    "def get_country(gpe):\n",
    "    geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
    "    location = geolocator.geocode(gpe)\n",
    "    if location:\n",
    "        loc_lst = location.address.split(',')\n",
    "        return loc_lst[-1]\n",
    "    return None\n",
    "\n",
    "# hlpr func: return a list of countries names\n",
    "def countries():\n",
    "    return list(map(lambda x: x.name, list(pycountry.countries)))\n",
    "\n",
    "# hlpr func: return True if name countains country name\n",
    "def contain_country(word, ctry_lst):\n",
    "    for ctry in ctry_lst:\n",
    "        if ctry.lower() in word.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# hlpr func: extract entities with tag 'GPE', 'ORG', 'NORP'\n",
    "def search_target_ent(tags):\n",
    "    country_lst = countries()\n",
    "    tag_lst = []\n",
    "    for i in range(len(tags)):\n",
    "        if tags[i][1] == 'GPE' or tags[i][1] == 'ORG' or tags[i][1] == 'NORP':\n",
    "            if contain_country(tags[i][0], country_lst):\n",
    "                tag_lst.append(tags[i])\n",
    "    return tag_lst\n",
    "\n",
    "# hlpr func: return the odd of the person's nationality in the article is nat\n",
    "def calc_odd_nationality(nat,lst):\n",
    "    try:\n",
    "        result = []\n",
    "        for tag in lst:\n",
    "            if tag[0] is not None and nat is not None:\n",
    "                if nat.lower() in tag[0].lower():\n",
    "                    result.append(tag)\n",
    "                    continue\n",
    "            try:\n",
    "                if tag[1] == 'GPE' and (get_country(tag[0]) is not None and nat is not None):\n",
    "                    if get_country(tag[0]).lower() == nat.lower():\n",
    "                        result.append(tag)\n",
    "            except GeocoderServiceError as e:\n",
    "                pass\n",
    "        prob = 1 if ((len(lst) - len(result)) == 0 and len(result) > 0) else (len(result) / (len(lst) - len(result)))\n",
    "        prob = 1 if prob > 1 else prob\n",
    "        return prob\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "\n",
    "# hlpr func: return True if name fuzzy matching score > 80\n",
    "def is_target(name, article_name):\n",
    "    return fuzz.partial_ratio(name, article_name) > 80\n",
    "\n",
    "# the main function for nationality matching\n",
    "# return odd if target tags found else return 0\n",
    "def nationality_matching(tags, nationality, person):\n",
    "    \n",
    "    if nationality is None:\n",
    "        return None\n",
    "    \n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "            if tags[i][1] == 'PERSON':\n",
    "                \n",
    "                # check if is target\n",
    "                if is_target(person, tags[i][0]):\n",
    "                    search = search_target_ent(tags)\n",
    "                \n",
    "                    if len(search) != 0:\n",
    "                        return calc_odd_nationality(nationality, search)\n",
    "        return 0\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "\n",
    "# hlpr func: parse text to tags\n",
    "def parse(text):\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "# hlpr func: return True if token is a name and subject\n",
    "def is_name_subj(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ == 'nsubjpass')  and token.pos_ == 'PROPN'\n",
    "\n",
    "def is_part_of_name(token):\n",
    "    return (token.dep_ =='nsubj' or token.dep_ =='compound' or token.dep_ == 'nsubjpass') \\\n",
    "        and token.pos_ == 'PROPN'\n",
    "\n",
    "# hlpr func: return True if the token is a determiner: his, her, hers\n",
    "def is_det(token):\n",
    "    return token.pos_ == 'DET' and (token.dep_ == 'poss' or token.dep_ == 'attr')\n",
    "\n",
    "# hlpr func: return True if the token is a pronoun: he, she, herself, himself\n",
    "def is_pron(token):\n",
    "    return token.pos_ == 'PRON' and \\\n",
    "        (token.dep_ == 'nsubj' or token.dep_ == 'nsubjpass' or token.dep_ == 'pobj' or token.dep_ == 'dobj')\n",
    "\n",
    "# hlpr func: return True if the gender noun is referring to target person\n",
    "def refer_target(gender, noun, name, text):\n",
    "    m = ['man', 'boy', 'guy']\n",
    "    f = ['woman', 'lady', 'girl']\n",
    "    \n",
    "    if is_target(name, text):\n",
    "        return (gender == 'male' and noun in m) or (gender == 'female' and noun in f)\n",
    "    return 0\n",
    "\n",
    "# hlpr func: return True if gender noun is follwed by 'is, was, as or comma'\n",
    "def gender_noun(t1, t2):\n",
    "    gender_nouns = ['man', 'boy', 'guy', 'woman', 'lady', 'girl']\n",
    "    verbs = ['was', 'is', 'as', ',']\n",
    "    return (t1 in gender_nouns) and (t2 in verbs)\n",
    "\n",
    "# hlpr func: return the probability of the gender in article to the true gender\n",
    "def calc_prob_gender(pron_lst, gender):\n",
    "    male_pron = ['he', 'his', 'himself', 'him']\n",
    "    female_pron = ['she', 'her', 'herself', 'hers']\n",
    "    n_target = 0\n",
    "    gdr_pron = []\n",
    "    \n",
    "    if gender.lower() == 'male':\n",
    "        gdr_pron = male_pron\n",
    "    else:\n",
    "        gdr_pron = female_pron\n",
    "        \n",
    "    for pron in pron_lst:\n",
    "        if pron in gdr_pron:\n",
    "            n_target += 1\n",
    "    return n_target / len(pron_lst) if len(pron_lst) else 0\n",
    "\n",
    "# the main function in gender matching\n",
    "def gender_matching(text, gender, name):\n",
    "    \n",
    "    if gender is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        pron_lst = ['he', 'his', 'himself', 'him', 'she', 'her', 'herself', 'hers']\n",
    "        name_str = ''\n",
    "        target_name = name.replace(\" \", \"\")\n",
    "        target_found = False\n",
    "        res_lst = []\n",
    "        \n",
    "        # text tagging\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(doc):\n",
    "\n",
    "            # catch text like '...woman is/was/as/, xxx...'\n",
    "            if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                    return (1)\n",
    "\n",
    "            # search for target name of subject form\n",
    "            if is_name_subj(doc[i]):\n",
    "                end_name = i\n",
    "                start_name = i\n",
    "                while is_part_of_name(doc[start_name]):\n",
    "                    start_name -= 1\n",
    "                start_name += 1\n",
    "                while start_name <= end_name:\n",
    "                    name_str += doc[start_name].text\n",
    "                    start_name += 1\n",
    "\n",
    "            if is_target(name_str, target_name):\n",
    "                target_found = True\n",
    "            else:\n",
    "                name_str = ''\n",
    "                target_found = False\n",
    "          \n",
    "            # if target name is found, search for pronouns, break if another name is found\n",
    "            while target_found:\n",
    "                i+=1\n",
    "                if gender_noun(doc[i].text, doc[i + 1].text):\n",
    "                    if refer_target(gender.lower(), doc[i].text, name, doc[i + 2].text):\n",
    "                        return (1)\n",
    "                if is_name_subj(doc[i]):\n",
    "                    target_found = False\n",
    "                    name_str = ''\n",
    "                    break\n",
    "                if is_det(doc[i]) or is_pron(doc[i]):\n",
    "                    if (doc[i].text).lower() in pron_lst:\n",
    "                        res_lst.append((doc[i].text).lower())\n",
    "                        break\n",
    "\n",
    "            i += 1\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    return calc_prob_gender(res_lst, gender)\n",
    "\n",
    "useless_dates = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday','yesterday','today']\n",
    "\n",
    "#index is index of person\n",
    "def forward_searcher(index,tags):\n",
    "    for i in range(index,len(tags)):\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "    return [None,None]\n",
    "\n",
    "def backward_searcher(index,tags):\n",
    "    i = index\n",
    "    while i >= 0:\n",
    "        if tags[i][1] == 'DATE' and tags[i][0] not in useless_dates:\n",
    "            return tags[i]\n",
    "        else:\n",
    "            i -=1\n",
    "\n",
    "def detect_age(age,lst):\n",
    "    try:\n",
    "        if lst[1] is not None and lst[2] is not None:\n",
    "            date1 = lst[1][0]\n",
    "            date2 = lst[2][0]\n",
    "            if (str(age) in date1) or (str(age) in date2):\n",
    "                return True\n",
    "        else:\n",
    "\n",
    "            if lst[1] == None:\n",
    "                if str(age) in lst[2][0]:\n",
    "                    return True\n",
    "\n",
    "            if lst[2] == None:\n",
    "                if str(age) in lst[1][0]:\n",
    "                    return True\n",
    "    except TypeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def confirm_age(lst,age,threshold):\n",
    "    iterating_lst = []\n",
    "    plus = 1\n",
    "    minus = -1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+plus)\n",
    "        plus += 1\n",
    "    for i in range(threshold):\n",
    "        iterating_lst.append(age+minus)\n",
    "        minus -=1 \n",
    "    iterating_lst.append(age)\n",
    "    \n",
    "    for j in iterating_lst:\n",
    "        if str(j) in lst[1][0]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def age_matching(name_dict,tags,age):\n",
    "    '''\n",
    "    tags: parse(text)\n",
    "    age: desired age to check\n",
    "    '''\n",
    "    if age is None:\n",
    "        return None\n",
    "    \n",
    "    for tag in tags:\n",
    "        #if tag[1] == 'DATE':\n",
    "            #print(tag)\n",
    "        if str(age) in tag[0] or str(age+1) in tag[0] or str(age-1) in tag[0]:\n",
    "            return 1\n",
    "    result = []\n",
    "    try:\n",
    "        for i in range(len(tags)):\n",
    "            #if second item is a name\n",
    "\n",
    "            \n",
    "            if tags[i][1] == 'PERSON':\n",
    "                if tags[i][0] in name_dict:\n",
    "\n",
    "                    forward_age = forward_searcher(i,tags)\n",
    "                    backwards_age = backward_searcher(i,tags)\n",
    "                    new_list = [tags[i],forward_age,backwards_age]\n",
    "                    #new_list = [tags[i-1],tags[i],tags[i+1]]\n",
    "                    #print(new_list)\n",
    "\n",
    "                    if detect_age(age,new_list) and tags[i][0] in name_list:\n",
    "\n",
    "                        #print(new_list)\n",
    "                        #result += new_list\n",
    "\n",
    "                        if str(age) in new_list[1][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i], new_list[1]])\n",
    "                            return(confirm_age([tags[i],new_list[1]],age,3))\n",
    "\n",
    "\n",
    "                        elif str(age) in new_list[2][0]:\n",
    "                            #print('****************')\n",
    "                            #print([tags[i],new_list[2]])\n",
    "                            return(confirm_age([tags[i],new_list[2]],age,3))\n",
    "                        \n",
    "        return 0\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "    \n",
    "def entity_recognition_scoring_each_article(input_info, text, names_list):\n",
    "    output = []\n",
    "    input_name = input_info['name']\n",
    "\n",
    "\n",
    "    article_names_list = names_list.most_common() \n",
    "    matched = False\n",
    "\n",
    "    for each_name, each_count in article_names_list: ## as of now checking all names within the article, should we limit to e.g. top 3/5?\n",
    "        if len(each_name.split()) == 1 and each_name in input_name:\n",
    "            score = 100 ## if surname matches, default match score 100 \n",
    "        else: \n",
    "            try: \n",
    "                score = ER_name_matching(input_name, each_name)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "        if score is not None:\n",
    "            matched = True\n",
    "        if matched:\n",
    "            break\n",
    "    conf_score = 0\n",
    "    if matched:\n",
    "        name_score = score\n",
    "        nationality_score = nationality_matching(parse(text), input_info['nationality'], input_info['name'])\n",
    "        gender_score = gender_matching(text, input_info['gender'], input_info['name'])\n",
    "        age_score = age_matching(names_list,parse(text),input_info['year_of_birth'])\n",
    "            \n",
    "        conf_score = (0.9071/100 * name_score) + (0.049973 * nationality_score) + (0.030293 * gender_score) + (0.012634 * age_score) \n",
    "    return conf_score\n",
    "    \n",
    "# Main Function\n",
    "def search_articles_on_individual(individual_dict, no_of_articles=30):\n",
    "    def generate_link(person_dict, attributes_used = ['name'], keywords=['crimes', 'sentenced']):\n",
    "        link_start = \"https://www.google.com/search?q=\"\n",
    "        link_end = \"&sxsrf=ALeKk01K1bOuJFHjy4HBARo1cRpUYakYPg:1629640327633&source=lnms&tbm=nws&sa=X&sqi=2&ved=2ahUKEwiu29um48TyAhWGqpUCHYuoAlcQ_AUoAnoECAEQBA&biw=1441&bih=718&dpr=2\" \n",
    "        link_query = \"\"\n",
    "\n",
    "        for attributes in attributes_used:\n",
    "            temp_attr = person_dict[attributes]\n",
    "            if temp_attr is not None:\n",
    "                temp_attr = str(temp_attr)\n",
    "                link_query += temp_attr.replace(' ', '+') + '+'       \n",
    "                \n",
    "        links = []\n",
    "        for keyword in keywords:\n",
    "            temp_search_link = link_start + link_query + keyword + link_end + \"&num=\" + str(no_of_articles)\n",
    "            links.append(temp_search_link)\n",
    "        return links\n",
    "    \n",
    "    def article_extraction(link):\n",
    "        article = Article(link)\n",
    "        article.download()\n",
    "        try:\n",
    "            article.parse()\n",
    "        except:\n",
    "            pass\n",
    "        return article.text\n",
    "\n",
    "    def parse(text):\n",
    "        #try:     \n",
    "        doc = nlp(text)\n",
    "        tags = [[X.text, X.label_] for X in doc.ents]\n",
    "        labels = [x.label_ for x in doc.ents]\n",
    "        items = [x.text for x in doc.ents]\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def find_names(tags):\n",
    "        names = []\n",
    "        for tag in tags:\n",
    "            if tag[1] == 'PERSON':\n",
    "                names.append(tag[0])\n",
    "        return names\n",
    "    \n",
    "    def time_to_months(time):\n",
    "        if 'weeks' in time:\n",
    "            return 0\n",
    "        else:\n",
    "            return int(time.split(' month')[0])\n",
    "\n",
    "    search_links = generate_link(individual_dict)\n",
    "    \n",
    "    unique_links_checker = []\n",
    "    \n",
    "    output = []\n",
    "    for x in search_links:\n",
    "        req = Request(x, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "        webpage = urlopen(req).read()\n",
    "\n",
    "        with requests.Session() as c:\n",
    "            soup = BeautifulSoup(webpage, 'html5lib')\n",
    "            #print(soup)\n",
    "            for item in soup.find_all('div', attrs = {'class': \"ZINbbc xpd O9g5cc uUPGi\"}):\n",
    "                current_dict = {}\n",
    "                raw_link = (item.find('a', href = True)['href'])\n",
    "                try:\n",
    "                    link = (raw_link.split(\"/url?q=\")[1]).split('&sa=U&')[0]\n",
    "                except IndexError as e1:\n",
    "                    continue\n",
    "                if link not in unique_links_checker and item:\n",
    "                    unique_links_checker.append(link)\n",
    "                    title = item.find('div',attrs = {'class': 'BNeawe vvjwJb AP7Wnd'})\n",
    "                    if title == None:\n",
    "                        continue\n",
    "                    title = title.get_text()\n",
    "                    description  = (item.find('div',attrs = {'class': 'BNeawe s3v9rd AP7Wnd'}).get_text())\n",
    "                    time = description.split(\" · \")[0]\n",
    "                    #print(description)\n",
    "                    descript = description.split(\" · \")[1]\n",
    "                    \n",
    "                    # create names_list\n",
    "                    parsed_description = parse(description)\n",
    "                    names_in_description = find_names(parsed_description) \n",
    "                    parsed_text = parse(article_extraction(link))\n",
    "                    names_in_text = find_names(parsed_text)\n",
    "                    names_list = Counter(names_in_description + names_in_text)\n",
    "                    \n",
    "                    # extract text\n",
    "                    text = article_extraction(link)\n",
    "\n",
    "                    # compute confidence score before accepting the article\n",
    "                    conf_score = entity_recognition_scoring_each_article(individual_dict, text, names_list)\n",
    "                    if conf_score < 0.95:\n",
    "                        continue\n",
    "                    current_dict['title'] = title\n",
    "                    current_dict['time'] = time\n",
    "                    try:\n",
    "                        current_dict['year_of_birth'] = (date.today() - relativedelta(months = time_to_months(time))).year - individual_dict['year_of_birth']\n",
    "                    except TypeError as e1:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    except ValueError as e2:\n",
    "                        current_dict['year_of_birth'] = 0\n",
    "                    current_dict['description'] = descript\n",
    "                    current_dict['link'] = link\n",
    "                    current_dict['text'] = text\n",
    "                    current_dict['names_list'] = names_list\n",
    "                    current_dict['confidence_score'] = conf_score\n",
    "                    \n",
    "                    output.append(current_dict)\n",
    "                else:\n",
    "                    pass\n",
    "    return output\n",
    "\n",
    "\n",
    "# Sentiment Analysis\n",
    "def sentiment_model(name_matched):\n",
    "    \n",
    "    if len(name_matched) == 0:\n",
    "        sys.exit(\"No Articles found.\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # Loading Model\n",
    "        reconstructed_model = keras.models.load_model(\"LSTM_GLOVE\")\n",
    "\n",
    "        url = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)\n",
    "        (?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([\n",
    "          ^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        def clean_data(temp):\n",
    "            temp = temp.map(lambda x:str(x).lower()) \n",
    "            # removing emails\n",
    "            temp = temp.map(lambda x:re.sub(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\", \"\", x)) \n",
    "            # removing url\n",
    "            temp = temp.map(lambda x:re.sub(url, \"\", x)) \n",
    "            # removing numbers\n",
    "            temp = temp.map(lambda x:re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', \"\", x)) \n",
    "            # removing white space\n",
    "            temp = temp.map(lambda x:re.sub(r'^\\s*|\\s\\s*', ' ', x).strip()) \n",
    "            # removing punctuations\n",
    "            temp = temp.map(lambda x:''.join([c for c in x if c not in string.punctuation])) \n",
    "            # removing special characters\n",
    "            temp = temp.map(lambda x:re.sub(r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]', '', x)) \n",
    "            # unicode\n",
    "            temp = temp.map(lambda x:unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')) \n",
    "            # tokenising text for cleaning\n",
    "            temp = temp.map(lambda x:tokenizer.tokenize(x)) \n",
    "            # removing stop words\n",
    "            temp = temp.map(lambda x:[i for i in x if i not in stopwords.words('english')]) \n",
    "            temp = temp.map(lambda x:' '.join(x))\n",
    "            return temp\n",
    "\n",
    "        name_matched['body'] = name_matched['text']\n",
    "        name_matched.text = clean_data(name_matched.text)\n",
    "\n",
    "        # Data Preprocessing for model ingestion\n",
    "        maxlen = 50\n",
    "        embedding_dim = 100\n",
    "\n",
    "        X = name_matched.text.values\n",
    "        tokenizer = Tokenizer(num_words=5000)\n",
    "        tokenizer.fit_on_texts(name_matched.text.values)\n",
    "        X = tokenizer.texts_to_sequences(X)\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        test_input = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
    "\n",
    "        # Predicting output\n",
    "        test = reconstructed_model.predict(test_input)\n",
    "        test_classes = np.argmax(test,axis=1)\n",
    "        name_matched['prediction'] = test_classes\n",
    "\n",
    "        def predicted_classes(df):\n",
    "            val = ''  \n",
    "            if df['prediction'] == 0:\n",
    "                val = 'negative'\n",
    "            elif df['prediction'] == 1:\n",
    "                val = 'neutral'\n",
    "            else:\n",
    "                val = 'positive'\n",
    "\n",
    "            return val\n",
    "\n",
    "        name_matched['sentiment'] = name_matched.apply(predicted_classes,axis=1)\n",
    "        name_matched = name_matched[['title', 'time', 'year_of_birth', 'description', 'link', 'body',\n",
    "                                       'names_list', 'confidence_score', 'sentiment']]\n",
    "    \n",
    "        return name_matched\n",
    "\n",
    "def demo():\n",
    "    # Web Scraping\n",
    "    df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    df_dict = preprocess_df_to_dict(df)\n",
    "    test_record_1 = df_dict[4]\n",
    "    \n",
    "    print('Test input: \\n')\n",
    "    print('Name: ' + str(test_record_1['name']))\n",
    "    print('Alias: ' + str(test_record_1['alias']))\n",
    "    print('Year of Birth: ' + str(test_record_1['year_of_birth']))\n",
    "    print('Month of Birth: ' + str(test_record_1['month_of_birth']))\n",
    "    print('Day of Birth: ' + str(test_record_1['day_of_birth']))\n",
    "    print('Gender: ' + str(test_record_1['gender']))\n",
    "    print('Nationality: ' + str(test_record_1['nationality']))\n",
    "    print('Actual Name: ' + str(test_record_1['actual_name']))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    test_query = search_articles_on_individual(test_record_1, 10)\n",
    "    test_query = pd.DataFrame(test_query)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    output = sentiment_model(test_query)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \n",
      "\n",
      "Name: Joe Cho\n",
      "Alias: None\n",
      "Year of Birth: None\n",
      "Month of Birth: None\n",
      "Day of Birth: None\n",
      "Gender: None\n",
      "Nationality: South Korea\n",
      "Actual Name: Joe Cho\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-bfabb08e1d75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtest_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_articles_on_individual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_record_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtest_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-948808b29c74>\u001b[0m in \u001b[0;36msearch_articles_on_individual\u001b[0;34m(individual_dict, no_of_articles)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0;31m# compute confidence score before accepting the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                     \u001b[0mconf_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_recognition_scoring_each_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconf_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-948808b29c74>\u001b[0m in \u001b[0;36mentity_recognition_scoring_each_article\u001b[0;34m(input_info, text, names_list)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mname_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mnationality_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnationality_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nationality'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mgender_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgender_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0mage_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mage_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year_of_birth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-948808b29c74>\u001b[0m in \u001b[0;36mgender_matching\u001b[0;34m(text, gender, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalc_prob_gender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0museless_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Monday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tuesday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Wednesday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Thursday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Friday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Saturday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sunday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'yesterday'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'today'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-948808b29c74>\u001b[0m in \u001b[0;36mcalc_prob_gender\u001b[0;34m(pron_lst, gender)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mgdr_pron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'male'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mgdr_pron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmale_pron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"Shopee Test.xlsx\", engine=\"openpyxl\")\n",
    "df = df.where(pd.notnull(df), None)\n",
    "df_dict = preprocess_df_to_dict(df)\n",
    "test_record_1 = df_dict[6]\n",
    "\n",
    "print('Test input: \\n')\n",
    "print('Name: ' + str(test_record_1['name']))\n",
    "print('Alias: ' + str(test_record_1['alias']))\n",
    "print('Year of Birth: ' + str(test_record_1['year_of_birth']))\n",
    "print('Month of Birth: ' + str(test_record_1['month_of_birth']))\n",
    "print('Day of Birth: ' + str(test_record_1['day_of_birth']))\n",
    "print('Gender: ' + str(test_record_1['gender']))\n",
    "print('Nationality: ' + str(test_record_1['nationality']))\n",
    "print('Actual Name: ' + str(test_record_1['actual_name']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test_query = search_articles_on_individual(test_record_1, 10)\n",
    "test_query = pd.DataFrame(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input: \n",
      "\n",
      "Name: Sheng Yang Tay\n",
      "Alias: None\n",
      "Year of Birth: None\n",
      "Month of Birth: None\n",
      "Day of Birth: None\n",
      "Gender: Male\n",
      "Nationality: Singapore\n",
      "Actual Name: Tay Sheng Yang\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eee72ae13cc7>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m     \u001b[0mtest_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_articles_on_individual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_record_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m     \u001b[0mtest_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eee72ae13cc7>\u001b[0m in \u001b[0;36msearch_articles_on_individual\u001b[0;34m(individual_dict, no_of_articles)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                     \u001b[0;31m# compute confidence score before accepting the article\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                     \u001b[0mconf_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_recognition_scoring_each_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindividual_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconf_score\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eee72ae13cc7>\u001b[0m in \u001b[0;36mentity_recognition_scoring_each_article\u001b[0;34m(input_info, text, names_list)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mage_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mage_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year_of_birth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mconf_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9071\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mname_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.049973\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnationality_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.030293\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgender_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.012634\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mage_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconf_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "demo()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a41f48461941a5aefc82610e94371dafd9500cffa630c3ca9566e477af78c3ed"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
