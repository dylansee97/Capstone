{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import math\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "import random\n",
    "import csv\n",
    "import seleniumwire\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-b7b4d3591f36>:5: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(executable_path=driver_path, chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable-automation\"])\n",
    "driver_path = '/Users/dylan/Desktop/Y4S1/BT4103/scraper/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=driver_path, chrome_options=options)\n",
    "driver.delete_all_cookies()\n",
    "driver.set_window_position(0,0)\n",
    "driver.get('https://www.google.com/')\n",
    "ID = 'Donald Trump'\n",
    "driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input').send_keys(ID)\n",
    "driver.find_element_by_xpath('/html/body/div[1]/div[3]/form/div[1]/div[1]/div[1]/div/div[2]/input').send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import URL csv that contains the list of all URLs to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('url.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    DATA = list(reader)\n",
    "\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Read in ID and password for quick login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"leowsy@comfortdelgrobus.com.sg\"\n",
    "password = \"Cdgbmar21++\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Open chrome driver and navigate Tenderboard to deals page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "options.add_experimental_option(\"excludeSwitches\",[\"enable-automation\"])\n",
    "driver_path = '/Users/dylan/comfort/Tender Prediction/chromedriver'\n",
    "driver = webdriver.Chrome(executable_path=driver_path, chrome_options=options)\n",
    "driver.delete_all_cookies()\n",
    "driver.set_window_position(0,0)\n",
    "driver.get('https://www.tenderboard.biz/login')\n",
    "driver.find_element_by_id('edit-name').send_keys(ID)\n",
    "driver.find_element_by_id('edit-pass').send_keys(password)\n",
    "driver.find_element_by_xpath('//*[@id=\"user-login\"]/div/div[4]/input').click()\n",
    "time.sleep(5)\n",
    "driver.find_element_by_xpath('//*[@id=\"menu-tenders\"]').click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Define helper helper functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interceptor(request, ref):\n",
    "    #def interceptor (request, referer_value)\n",
    "    '''\n",
    "    Changes the referer website to Tenderboard to prevent detection\n",
    "    '''\n",
    "    del request.headers['referer']  \n",
    "    request.headers['referer'] = 'https://www.tenderboard.biz/tenders' \n",
    "    \n",
    "def format_names(names,company_names):\n",
    "    '''\n",
    "    filter out company names only from the list of names in the div and appends\n",
    "    to company_names to store all the company names\n",
    "    '''\n",
    "    for i in range(len(names)):\n",
    "        word = names[i].text\n",
    "        if word != '' and 'cipant' not in word:\n",
    "            company_names.append(word)\n",
    "    return company_names\n",
    "            \n",
    "def format_prices(nums,prices):\n",
    "    '''\n",
    "    filter out only the prices and stores into prices\n",
    "    '''\n",
    "    for i in range(len(nums)):\n",
    "        word = nums[i].text\n",
    "        if word != '' and len(word) <20:\n",
    "            prices.append(word)\n",
    "    return(prices)\n",
    "\n",
    "\n",
    "def format_bids(prices, num_of_bid, bid_list):\n",
    "    '''\n",
    "    arranges the bids into lists for the individual respective companies\n",
    "    '''\n",
    "    temp_array = []\n",
    "    for i in range (len(prices)):\n",
    "        if len(temp_array) == num_of_bid:\n",
    "            bid_list.append(temp_array)\n",
    "            temp_array = []\n",
    "            temp_array.append(prices[i])\n",
    "        else:\n",
    "            temp_array.append(prices[i])\n",
    "\n",
    "        if len(temp_array) == num_of_bid:\n",
    "            bid_list.append(temp_array)\n",
    "            temp_array = []\n",
    "    return bid_list\n",
    "\n",
    "def add_error_to_log(url, error):\n",
    "    '''\n",
    "    logs the error that was met during scraping into a dictionary.\n",
    "    the log will be of the format {'url1': [error1, error2,...]}\n",
    "    '''\n",
    "    global ERROR_LOG\n",
    "    if url not in ERROR_LOG.keys():\n",
    "        ERROR_LOG[url] = [error]\n",
    "    else:\n",
    "        ERROR_LOG[url].append(error)\n",
    "        \n",
    "        \n",
    "def get_industry(lst):\n",
    "    '''\n",
    "    parses in selenium element and splits it into a list of elements.\n",
    "    iteratively selects line items industry from the list.\n",
    "    '''\n",
    "    industry = ''\n",
    "    line = []\n",
    "    lst = lst[0].text.split('\\n')\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i].startswith('Industry:'):\n",
    "            industry = lst[i+1]\n",
    "    return industry \n",
    "\n",
    "def get_line(lst):\n",
    "    '''\n",
    "    parses in selenium element and splits it into a list of elements.\n",
    "    iteratively selects line items from the list.\n",
    "    '''\n",
    "    industry = ''\n",
    "    line = []\n",
    "    lst = lst[0].text.split('\\n')\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i].startswith('Line Items:'):  \n",
    "            for j in range(i+1,len(lst)):\n",
    "                if lst[j].startswith('Item '):\n",
    "                    line.extend(lst[j].split('\\n'))\n",
    "                elif lst[j].startswith('Files '):\n",
    "                    break\n",
    "    return line\n",
    "\n",
    "def get_line_items():\n",
    "\n",
    "    '''\n",
    "    get_line_items gets the div from the website that contains all the line items\n",
    "    the div then allocates the button header attribute if the button header is present\n",
    "    if or otherwise, the scraper will carry out different scraping algorithms that will extract a list\n",
    "    of all the line items in the div\n",
    "    '''\n",
    "    randwait = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    randwait2 = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    randwait3 = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    more_details_class = driver.find_elements_by_xpath('//*[@id=\"tender-more-detail\"]/div')\n",
    "    button_header = None\n",
    "    for i in more_details_class:\n",
    "        class_value = i.get_attribute('class')\n",
    "        if class_value == 'mdl-cell mdl-cell--10-col-desktop mdl-cell--hide-tablet mdl-cell--hide-phone':\n",
    "\n",
    "            button_header = i\n",
    "\n",
    "    selenium_list_items = []\n",
    "    \n",
    "    #if there is button (more than 4 line items):\n",
    "    if button_header != None:\n",
    "        for i in range(1,10):\n",
    "            time.sleep(1)\n",
    "            print(i)\n",
    "            try:\n",
    "                time.sleep(randwait)\n",
    "                button_header2 = button_header.find_element_by_xpath('div')\n",
    "                button_header3 = button_header2.find_element_by_xpath('button')\n",
    "                time.sleep(randwait2)\n",
    "                button_header3.click()\n",
    "                time.sleep(randwait3)\n",
    "                driver.find_element_by_xpath(f'//*[@id=\"tender-more-detail\"]/div[6]/div/div/ul/li[{i}]').click()\n",
    "                lst = driver.find_elements_by_xpath('//*[@id=\"tender-more-detail\"]')\n",
    "                selenium_list_items.append(get_line(lst))\n",
    "                \n",
    "            except NoSuchElementException as nse:\n",
    "                break\n",
    "    else:\n",
    "        lst = driver.find_elements_by_xpath('//*[@id=\"tender-more-detail\"]')\n",
    "        selenium_list_items.append(get_line(lst))\n",
    "    return flatmap(selenium_list_items)\n",
    "\n",
    "def get_page_bids(bid_items):\n",
    "    bid_items = bid_items[0]\n",
    "            \n",
    "    bid_prices_list = []\n",
    "    \n",
    "    bid_items = bid_items.text\n",
    "    bid_items = bid_items.replace(',', '')\n",
    "    bid_items = bid_items.replace('\\n', 'mmm').split('mmm')\n",
    "    \n",
    "    for item in bid_items:\n",
    "        try:\n",
    "            flt = float(item)\n",
    "            bid_prices_list.append(flt)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    return bid_prices_list\n",
    "\n",
    "\n",
    "def get_bid_items():\n",
    "    randwait = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    randwait2 = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    randwait3 = random.randrange(1,2) + random.choices([0.1,0.2,0.25])[0]\n",
    "    button_items = driver.find_elements_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[6]/div')\n",
    "    button_header = None\n",
    "    \n",
    "    for i in button_items:\n",
    "        class_value = i.get_attribute('class')\n",
    "        #print(class_value)\n",
    "        if class_value == 'mdl-cell mdl-cell--4-offset-desktop mdl-cell--8-col-desktop mdl-cell--hide-tablet mdl-cell--hide-phone':\n",
    "            #mdl-cell mdl-cell--2-offset-tablet mdl-cell--6-col-tablet mdl-cell--hide-desktop mdl-cell--hide-phone\n",
    "            button_header = i\n",
    "            \n",
    "    bid_list = []\n",
    "    \n",
    "        #if there is button (more than 4 line items):\n",
    "    if button_header != None:\n",
    "        for i in range(1,10):\n",
    "            time.sleep(1)\n",
    "            print(i)\n",
    "            try:\n",
    "                time.sleep(randwait)\n",
    "                button_header2 = button_header.find_element_by_xpath('div')\n",
    "                button_header3 = button_header2.find_element_by_xpath('button')\n",
    "                time.sleep(randwait2)\n",
    "                button_header3.click()\n",
    "                time.sleep(randwait3)\n",
    "                driver.find_element_by_xpath(f'//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[6]/div[4]/div/div/ul/li[{i}]').click()\n",
    "                #div that contains the names of the bidders, and the price\n",
    "                bid_items = driver.find_elements_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[6]')\n",
    "                page_bids = get_page_bids(bid_items)\n",
    "                bid_list.append(page_bids)\n",
    "                \n",
    "            except NoSuchElementException as nse:\n",
    "                break\n",
    "                \n",
    "    else:\n",
    "        bid_items = driver.find_elements_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[6]')\n",
    "        page_bids = get_page_bids(bid_items)\n",
    "        bid_list.append(page_bids)\n",
    "    \n",
    "    return flatmap(bid_list)\n",
    "\n",
    "\n",
    "def flatmap(lst):\n",
    "    final = []\n",
    "    for item in lst:\n",
    "        for item2 in item:\n",
    "            final.append(item2)\n",
    "    return final\n",
    "\n",
    "\n",
    "def organise(l, dim_r, dim_c,d):\n",
    "    '''\n",
    "    l, list of scraped numbers from site\n",
    "    dim_r, number of rows (total number of companies)\n",
    "    dim_c, number of columns (total number of bid items)\n",
    "    d, dictionary of of {company: ['bid1', 'bid2'....]}\n",
    "    \n",
    "    skip calculates the indexes which we need to skip,\n",
    "    which are the numbers that are not bids\n",
    "    \n",
    "    then, we append the rest of the bids to d, skipping\n",
    "    all unwanted indexes.\n",
    "    '''\n",
    "    #indexes where we skip adding the item\n",
    "    skip = int(len(l) / dim_r)\n",
    "    current = 0\n",
    "    processed_l = []\n",
    "    for i in range(len(l)):\n",
    "        if i % skip == 0:\n",
    "            continue\n",
    "        processed_l.append(l[i])\n",
    "\n",
    "    current_row = 0\n",
    "    while processed_l:\n",
    "        for i in range(dim_c):\n",
    "            d[current_row].append(processed_l.pop(0))\n",
    "        current_row += 1\n",
    "\n",
    "\n",
    "def resolve(l, num_rows, num_cols, d):\n",
    "    '''\n",
    "    recursive function that handles entire list,\n",
    "    updating number of columns\n",
    "    \n",
    "    if:\n",
    "    if the list l contains less than the number of rows*\n",
    "    (4+1) (4 bids per page, plus 1 unwanted variable per 4 bids)\n",
    "    \n",
    "    else:\n",
    "    if the length of l is more than num_rows*5,\n",
    "    we process only the first (num_rows*(4+1)) entries in l,\n",
    "    which are the number of entries for 1 page only.\n",
    "    \n",
    "    Then, we reduce the total number of columns by 4,\n",
    "    as we have already processed 4 bid items.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if len(l) <= num_rows*(4+1):\n",
    "        organise(l, num_rows, num_cols, d)\n",
    "    else:\n",
    "        organise(l[:num_rows*(4+1)], num_rows, 4, d)\n",
    "        resolve(l[num_rows*(4+1):], num_rows, num_cols-4, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Main scraping function that calls all previously defined functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    '''\n",
    "    scrape takes in the url (already navigated to by (3)). Scrapes all required values from each website by\n",
    "    calling the functions defined earlier. Try and except clauses will allow all errors to be logged under \n",
    "    ERROR_LOG. Returns a list of required values in the following format:\n",
    "    ['result', 'winners_bold', 'description', 'line_items', 'published_date', 'closing_date', 'closing_time', \n",
    "    'buyer_name', 'title','telephone', 'email','awarded_date', 'winner_text','awarded_amt','url']\n",
    "    \n",
    "    '''\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    names = soup.find_all(class_ ='mdl-cell mdl-cell--4-col mdl-cell--2-col-tablet mdl-cell--hide-phone')\n",
    "    #nums = soup.find_all(class_  = 'mdl-cell mdl-cell--1-col mdl-cell--hide-desktop mdl-cell--hide-phone')\n",
    "\n",
    "    winners_bold= ''\n",
    "    title = ''\n",
    "    items = ''\n",
    "    published_date = ''\n",
    "    closing_date = ''\n",
    "    closing_time = ''\n",
    "    indsutry = ''\n",
    "    tel = ''\n",
    "    email = ''\n",
    "    buyer_name = ''\n",
    "    line_items = ''\n",
    "    company_names = []\n",
    "    prices = []\n",
    "    bid_list = []\n",
    "    temp_array = []\n",
    "    result = {}\n",
    "    line = []\n",
    "    \n",
    "    try:\n",
    "        line = get_line_items()\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url,'line items')\n",
    "    \n",
    "    try:\n",
    "        winners_bold = soup.find(style=\"font-weight: bold;\")\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'winners_bold')\n",
    "\n",
    "    try:\n",
    "        title = driver.find_element_by_xpath('//*[@id=\"tender-desc\"]/div[1]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'title')\n",
    "    \n",
    "    try:\n",
    "        items = soup.find_all(class_  = 'mdl-cell mdl-cell--6-col mdl-cell--hide-desktop mdl-cell--hide-phone')\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'items')\n",
    "        \n",
    "    try:    \n",
    "        published_date = driver.find_element_by_xpath('//*[@id=\"tender-overview\"]/div[6]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'published date')\n",
    "\n",
    "    try:    \n",
    "        closing_date = driver.find_element_by_xpath('//*[@id=\"tender-overview\"]/div[9]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'closing date')\n",
    "        \n",
    "    try:    \n",
    "        closing_time = driver.find_element_by_xpath('//*[@id=\"tender-overview\"]/div[12]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'closing time')\n",
    "\n",
    "    try:     \n",
    "        industry = driver.find_element_by_xpath('//*[@id=\"tender-more-detail\"]/div[3]')\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'industry')\n",
    "\n",
    "    try:    \n",
    "        tel = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[4]/div[6]')\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'tel')\n",
    "    \n",
    "    try:\n",
    "        email = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[4]/div[9]')\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'email')\n",
    "\n",
    "    try:\n",
    "        header = driver.find_element_by_xpath('//*[@id=\"tender-desc\"]/div[1]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'header')\n",
    "\n",
    "    try:   \n",
    "        title = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[4]/div[3]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'title')\n",
    "\n",
    "    try:\n",
    "        awarded_date = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[7]/div[3]').text\n",
    "        winner_text = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[7]/div[6]').text\n",
    "        awarded_amt = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[2]/div[2]/div[1]/div[7]/div[9]').text\n",
    "    except Exception as e:\n",
    "        add_error_to_log(url, 'winner data')\n",
    "        \n",
    "    #number of line items \n",
    "    num_cols = len(line)\n",
    "    \n",
    "    prices = get_bid_items()\n",
    "   \n",
    "    company_names = format_names(names,company_names)\n",
    "    \n",
    "    num_rows = len(company_names)\n",
    "    \n",
    "    num_of_bid = len(prices)/len(company_names)\n",
    "    \n",
    "    bid_list = format_bids(prices, num_of_bid, bid_list)\n",
    "\n",
    "    buyer_name = driver.find_element_by_xpath('//*[@id=\"top-button-wrapper\"]/div[1]').text\n",
    "    \n",
    "    lst = driver.find_elements_by_xpath('//*[@id=\"tender-more-detail\"]')\n",
    "    \n",
    "    industry = get_industry(lst)\n",
    "    \n",
    "    item_values_by_company = {i:[] for i in range(num_rows)}\n",
    "    resolve(prices, num_rows, num_cols,item_values_by_company)\n",
    "    item_values_by_company = {company_names[k]:v for k,v in item_values_by_company.items()}\n",
    "\n",
    "    \n",
    "    final = [item_values_by_company, winners_bold.get_text(), industry, line, published_date, closing_date,closing_time, buyer_name, title,tel.text,email.text,awarded_date, winner_text,awarded_amt]\n",
    "    \n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Create data structure for the storage of scraped data\n",
    "## SCRAPED_DATA: list of lists that contains the data scraped from each website\n",
    "## SCRAPED_URLS: contains all urls that have been scraped for logging purposes\n",
    "## ERROR_LOG: logs urls with errors and the error faced\n",
    "## ROUND_COUNTER: the number of rounds of scraping done\n",
    "## please manually input round counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPED_DATA = []\n",
    "SCRAPED_URLS = []\n",
    "ERROR_LOG = {}\n",
    "ROUND_COUNTER = 4\n",
    "#DATAFRAME = pd.DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Scrape URLs by manually entering index. Scraped indexes will be logged seperately.\n",
    "## Scraped data will be added to SCRAPED_DATA\n",
    "## Scraped data does not automatically refresh. If you want to refresh,\n",
    "## please run previous cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(start_index, end_index):\n",
    "    '''\n",
    "    scrapes \n",
    "    '''\n",
    "    global ROUND_COUNTER\n",
    "    global SCRAPED_DATA\n",
    "    global ERROR_LOG\n",
    "    global SCRAPED_URLS\n",
    "    #global DATAFRAME\n",
    "    for i in range(start_index, end_index):\n",
    "        \n",
    "        time.sleep(2)\n",
    "        randwait = random.randrange(10,25) + random.choices([0.1,0.2,0.25])[0] + random.choices([0.37,0.48,0.63])[0]\n",
    "        randwait2 = random.randrange(9,14) + random.choices([0.1,0.2,0.25])[0] + random.choices([0.37,0.48,0.63])[0]\n",
    "        randwait3 = random.randrange(3,5) + random.choices([0.1,0.2,0.25])[0] + random.choices([0.37,0.48,0.63])[0]\n",
    "        time.sleep(randwait2)\n",
    "\n",
    "        url = DATA[i][0]\n",
    "        print(url)\n",
    "        driver.request_interceptor = interceptor\n",
    "        time.sleep(randwait3)\n",
    "        driver.get(url)\n",
    "\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(randwait3)\n",
    "        \n",
    "        try:\n",
    "            page_result = scrape(url)\n",
    "        except Exception as e:\n",
    "            print(url + \"has an error\")\n",
    "        page_result.append(url)\n",
    "        SCRAPED_DATA.append(page_result)\n",
    "        SCRAPED_URLS.append(url)\n",
    "\n",
    "        time.sleep(randwait)\n",
    "        print(SCRAPED_DATA)\n",
    "        print(i)\n",
    "        \n",
    "    print('done scraping')\n",
    "    fields = ['result', 'winners_bold', 'description', 'line_items', 'published_date', 'closing_date', 'closing_time', 'title','buyer_name', 'telephone', 'email','awarded_date', 'winner_text','awarded_amt','url','drop']\n",
    "    \n",
    "    pickle_name = f'scrape_{ROUND_COUNTER}'\n",
    "    \n",
    "    \n",
    "    #SCRAPED_DATA = []\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(SCRAPED_DATA, columns = fields)\n",
    "    df = df[df.columns[:15]]\n",
    "    df.to_pickle(pickle_name)\n",
    "    \n",
    "    log_data = SCRAPED_URLS.append(ERROR_LOG)\n",
    "    \n",
    "    #csv_name = f'scrape_18mar{ROUND_COUNTER}.csv'\n",
    "    with open(f'url_error_log{ROUND_COUNTER}.csv', 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        for item in SCRAPED_URLS:\n",
    "            write.writerow([item])\n",
    "    ROUND_COUNTER += 1\n",
    "    SCRAPED_URLS = []\n",
    "    ERROR_LOG = {}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Carry out scraping\n",
    "## Please manually input scraping URLS\n",
    "## This will start the scraping process and potentially get TB Account Banned.\n",
    "## Please DO NOT ANYHOW RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(ROUND_COUNTER)\n",
    "scrape_urls(100,110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After scraping please convert scraped data into .pkl file\n",
    "## Remember to run this code after each scraping round\n",
    "## to prevent data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(SCRAPED_DATA, columns = fields)\n",
    "df.to_pickle('scrape_3')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual function to scrape driver windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.switch_to.window(driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.switch_to.window(driver.window_handles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to help you to combine pkl files and read as pd DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(SCRAPED_DATA, columns = fields)\n",
    "df.to_pickle('scrape_4')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scrape_4', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "    \n",
    "with open('scrape_3', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "with open('scrape_2', 'rb') as f:\n",
    "    z = pickle.load(f)\n",
    "    \n",
    "with open('scrape_1', 'rb') as f:\n",
    "    w = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.columns = ['result', 'winners_bold', 'description', 'line_items', 'published_date', 'closing_date', 'closing_time', 'title','buyer_name', 'telephone', 'email','awarded_date', 'winner_text','awarded_amt','url']#,'drop','drop']\n",
    "z.columns = ['result', 'winners_bold', 'description', 'line_items', 'published_date', 'closing_date', 'closing_time', 'title','buyer_name', 'telephone', 'email','awarded_date', 'winner_text','awarded_amt','url']#,'drop','drop']\n",
    "w.columns = ['result', 'winners_bold', 'description', 'line_items', 'published_date', 'closing_date', 'closing_time', 'title','buyer_name', 'telephone', 'email','awarded_date', 'winner_text','awarded_amt','url']#,'drop','drop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([x,y], axis=0)\n",
    "df = pd.concat([df,z], axis = 0)\n",
    "df = pd.concat([df,w], axis = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_pickle('tenderboard_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[2][\"line_items\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[5][\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.iloc[1][\"winner_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['winner_text'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
